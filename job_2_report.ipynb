{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Exam Report @ UniBo a.y. 2023/2024\n",
    "\n",
    "- Manuel Andruccioli\n",
    "- Kelvin Olaiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":3, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures and definitions\n",
    "\n",
    "### Utility function for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T17:09:52.217563Z",
     "start_time": "2024-04-20T17:09:50.451214Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCharIndexes(line: String, char: Char): Seq[Int] = line.zipWithIndex.filter(_._1 == char).map(_._2) \n",
    "def splitAt(s: String, indices: Seq[Int]): Seq[String] = indices match {\n",
    "  case h +: t => s.splitAt(h) match {\n",
    "    case (a, b) => a +: splitAt(b, t.map(_ - h))\n",
    "  }\n",
    "  case Nil => Seq(s)\n",
    "}\n",
    "def parseCSVLine(l: String): Seq[String] = {\n",
    "  val apices = getCharIndexes(l, '\"').grouped(2).map { case Seq(a, b) => (a, b) }.toSeq\n",
    "  val commas = getCharIndexes(l, ',').filter(i => !apices.exists { case (a, b) => a < i && i < b })\n",
    "  return splitAt(l, commas).map(_.dropWhile(s => s == ',' || s == ' ')).map(_.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpersistRDDs() = sc.getPersistentRDDs.foreach(_._2.unpersist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:25:21.122005Z",
     "start_time": "2024-04-21T10:25:19.798787Z"
    }
   },
   "outputs": [],
   "source": [
    "case class Track(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  duration: Int,\n",
    "  explicit: Boolean,\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  album_uri: String,\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Tracks {\n",
    "  def fromCSVLine(line: String): Option[Track] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, duration, explicit, artists, available_markets, album_uri, popularity) =>\n",
    "        try {\n",
    "          Some(Track(uri, name, duration.toInt, explicit.toBoolean, artists, available_markets, album_uri, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Playlist(\n",
    "  pid: Int,\n",
    "  name: String,\n",
    "  num_follower: Int,\n",
    ")\n",
    "\n",
    "object Playlists {\n",
    "  def fromCSVLine(line: String): Option[Playlist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, name, num_follower) =>\n",
    "        try {\n",
    "            Some(Playlist(pid.toInt, name, num_follower.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class TrackInPlaylist(\n",
    "  pid: Int,\n",
    "  track_uri: String,\n",
    "  pos: Int,\n",
    ")\n",
    "\n",
    "object TrackInPlaylists {\n",
    "  def fromCSVLine(line: String): Option[TrackInPlaylist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, track_uri, pos) =>\n",
    "        try {\n",
    "          Some(TrackInPlaylist(pid.toInt, track_uri, pos.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Artist(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  followers: Int,\n",
    "  genres: String,             // List of genres, separated by |\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Artists {\n",
    "  def fromCSVLine(line: String): Option[Artist] =\n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, followers, genres, popularity) =>\n",
    "        try {\n",
    "          Some(Artist(uri, name, followers.toInt, genres, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Album(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  album_type: String,         // album, compilation, single.\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  release_year: String,\n",
    "  total_tracks: Int,\n",
    ")\n",
    "\n",
    "object Albums {\n",
    "  def fromCSVLine(line: String): Option[Album] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, album_type, artists, available_markets, release_year, total_tracks) =>\n",
    "        try {\n",
    "          Some(Album(uri, name, album_type, artists, available_markets, release_year, total_tracks.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Feature(\n",
    "  uri: String,\n",
    "  key: Int,\n",
    "  loudness: Double,\n",
    "  tempo: Double,\n",
    "  mode: Boolean,\n",
    "  danceability: Double,\n",
    "  valence: Double,\n",
    "  instrumentalness: Double,\n",
    "  liveness: Double,\n",
    "  acousticness: Double,\n",
    "  energy: Double,\n",
    "  speechiness: Double,\n",
    ")\n",
    "\n",
    "object Features {\n",
    "  def fromCSVLine(line: String): Option[Feature] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, key, loudness, tempo, mode, danceability, valence, instrumentalness, liveness, acousticness, energy, speechiness) =>\n",
    "        try {\n",
    "          Some(Feature(uri, key.toInt, loudness.toDouble, tempo.toDouble, mode.toInt == 1, danceability.toDouble, valence.toDouble, instrumentalness.toDouble, liveness.toDouble, acousticness.toDouble, energy.toDouble, speechiness.toDouble))\n",
    "        } catch {\n",
    "          case e: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val bucketname = \"unibo-bd2324-olaiya\"\n",
    "\n",
    "val path_albums = \"s3a://\"+bucketname+\"/exam/albums.csv\"\n",
    "val path_artists = \"s3a://\"+bucketname+\"/exam/artists.csv\"\n",
    "val path_features = \"s3a://\"+bucketname+\"/exam/features.csv\"\n",
    "val path_playlists = \"s3a://\"+bucketname+\"/exam/playlists.csv\"\n",
    "val path_tracksInPlaylist = \"s3a://\"+bucketname+\"/exam/tracks_in_playlists.csv\"\n",
    "val path_tracks = \"s3a://\"+bucketname+\"/exam/tracks.csv\"\n",
    "\n",
    "sc.applicationId\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T17:24:25.863233Z",
     "start_time": "2024-04-20T17:24:17.899357Z"
    }
   },
   "outputs": [],
   "source": [
    "val datasetPath = \"dataset/\"\n",
    "\n",
    "val albumRdd = sc.textFile(path_albums).flatMap(Albums.fromCSVLine)\n",
    "val artistRdd = sc.textFile(path_artists).flatMap(Artists.fromCSVLine)\n",
    "val featureRdd = sc.textFile(path_features).flatMap(Features.fromCSVLine)\n",
    "val playlistRdd = sc.textFile(path_playlists).flatMap(Playlists.fromCSVLine)\n",
    "val trackInPlaylistRdd = sc.textFile(path_tracksInPlaylist).flatMap(TrackInPlaylists.fromCSVLine)\n",
    "val trackRdd = sc.textFile(path_tracks).flatMap(Tracks.fromCSVLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Date le seguenti metriche:\n",
    "  - popolarità della traccia\n",
    "  - popolarità media delle tracce in anno\n",
    "  - popolarità dell'artista (se più artisti, media di essi)\n",
    "Capire come una playlist viene influenzata maggiormente dalle precedenti metriche, mediando i valori delle tracce di cui è composta. Inoltre, aggregare le playlist sull'influenza precedentemente calcolata, mediando per il numero di followers delle playlist.\n",
    "La query permette di rispondere alla seguente domanda:\n",
    "una playlist influenzata maggiormente dalla popolarità delle tracce ha in media 500 followers. (stessa cosa per le altre due metriche di partenza)\n",
    " \n",
    "- Given the following classes: slowly danceable (tempo <= 130BPM, danceability > 0.5), swiftly danceable (tempo >130BPM, danceability > 0.5), slowly undanceable (tempo <= 130BPM, danceability <= 0.5), swiftly undanceable (tempo >130BPM, danceability <= 0.5); and the various keys (C, C#/Db, ...).\n",
    "  for each class and (key ---OR--- range of followers) get:\n",
    "    - The number of playlist.\n",
    "    - Average playlist's percentage.\n",
    "    - Percentage of explicit songs.\n",
    "    - Average number of playlist followers.\n",
    "    - Average tracks tempo\n",
    "    - Average tracks danceability\n",
    "  (The key of a playlist is the most present key among its tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 2\n",
    "\n",
    "Given the following classes: slowly danceable (tempo <= 130BPM, danceability > 0.5), swiftly danceable (tempo >130BPM, danceability > 0.5), slowly undanceable (tempo <= 130BPM, danceability <= 0.5), swiftly undanceable (tempo >130BPM, danceability <= 0.5); and the various keys (C, C#/Db, ...).\n",
    "  for each class and (key ---OR--- range of followers) get:\n",
    "  - The number of playlist.\n",
    "  - Average playlist's explicitness percentage.\n",
    "  - Average number of tracks in playlist.\n",
    "  - Average number of playlist followers.\n",
    "  <!-- - Average playlist danceability.\n",
    "  - Average playlist tempo. -->\n",
    "  (The key of a playlist is the most present key among its tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toClass(tempo: Double, danceablility: Double): String = (tempo, danceablility) match {\n",
    "  case (t, d) if t <= 130 && d > 0.5 => \"slowly danceable\"\n",
    "  case (t, d) if t > 130 && d > 0.5 => \"swiftly danceable\"\n",
    "  case (t, d) if t <= 130 && d <= 0.5 => \"slowly undanceable\"\n",
    "  case (t, d) if t > 130 && d <= 0.5 => \"swiftly undanceable\"\n",
    "}\n",
    "\n",
    "def toKey(key: Int): String = Seq(\"C\", \"C#/Db\", \"D\", \"D#/Eb\", \"E\", \"F\", \"F#/Gb\", \"G\", \"G#/Ab\", \"A\", \"A#/Bb\", \"B\")(key)\n",
    "\n",
    "def incrementKey(map: Map[Int, Int], key: Int) = {\n",
    "    val currentValue = map.getOrElse(key, 0)\n",
    "    map.updated(key, currentValue + 1)\n",
    "}\n",
    "\n",
    "def joinMap(map1: Map[Int, Int], map2: Map[Int, Int]): Map[Int, Int] = map1.map { case(k, v) => (k, map2.getOrElse(k, 0) + v) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val features = featureRdd.map(t => (t.uri, (t.tempo, t.danceability, t.key))).\n",
    "  join(trackRdd.map(t => (t.uri, t.explicit))).\n",
    "  map { case (uri, ((t, d, k), e)) => (uri, (t, d, k, e)) }\n",
    "\n",
    "val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid))\n",
    "\n",
    "val playlistWithFollowers = playlistRdd.map(p => (p.pid, p.num_follower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First implementation\n",
    "\n",
    "Following the self-join pattern we computed the `playlistClasses` rdd by joining the `tracksInPlaylist`'s rdd with the `features`'s one and then aggregating on the playlist ID (pid) in order to compute the average tempo and danceability of each playlist. \n",
    "After that, we joined `tracksInPlaylistWithFeatures` with `playlistWithFollowers` so that by aggregating on the **pid** we could compute the ratio of explict songs and the number of followers of each playlist. Finally, we joined this last result with `playlistClasses` so that we could calculate per each of those classes the number of playlist belonging to the class, the average of explicit song ratio and number of followers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute the class of each playlist\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features)\n",
    "\n",
    "val playlistClasses = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (t, d, k, _))) => (pid, (t, d, k)) }.\n",
    "    aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0))(\n",
    "        { case ((accT, accD, accK, c), (t, d, k)) => (accT + t, accD + d, incrementKey(accK, k), c + 1) },\n",
    "        { case ( (accT1, accD1, accK1, c1), (accT2, accD2, accK2, c2)) => (accT1 + accT2, accD1+ accD2, joinMap(accK1, accK2), c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accT, accD, accK, c) => (toKey(accK.maxBy(_._2)._1), toClass(accT/c, accD/c)) } //(pid, (k, cls))\n",
    "\n",
    "val job1 = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (_,_,_, e))) => (pid, if (e) 1 else 0) }.\n",
    "    join(playlistWithFollowers). // (pid, (e, num_followers)) --> consider --J&A-- or A&J\n",
    "    aggregateByKey((0, 0, 0))(\n",
    "        { case ((accE, nF, c), (e, f)) => (accE + e, f, c + 1)  },\n",
    "        { case ((accE1, nF, c1), (accE2, _, c2)) => (accE1 + accE2, nF, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, nF, c) => (accE/c, nF) }. // (pid, avgE, NF)\n",
    "    join(playlistClasses). // (pid ((avgE, NF), (k, cls)))\n",
    "    map { case (pid, ((avgE, nF), (k, cls))) => ((k, cls), (avgE, nF))}.\n",
    "    aggregateByKey((0.0, 0, 0))(\n",
    "        { case ((accE, accF, c), (e, f)) => (accE + e, accF + f, c+1) },\n",
    "        { case ((accE1, accF1, c1), (accE2, accF2, c2)) => (accE1 + accE2, accF1 + accF2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, accF, c) => (accE/c, accF/c, c) }\n",
    "    \n",
    "val result = job1.collect    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job completed in around `22 minutes`. This is surely due the size of the input data, in particular those regarding the tracks in playlist which is `8.6GB` large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step toward optimizing execution time we noticed that we accessed the result of the join between `tracksInPlaylist` and `features` multiple times. So we dediced to cache it in order to avoid loading the dataset more than once and thus avoiding waste of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute the class of each playlist\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).cache\n",
    "\n",
    "val playlistClasses = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (t, d, k, _))) => (pid, (t, d, k)) }.\n",
    "    aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0))(\n",
    "        { case ((accT, accD, accK, c), (t, d, k)) => (accT + t, accD + d, incrementKey(accK, k), c + 1) },\n",
    "        { case ( (accT1, accD1, accK1, c1), (accT2, accD2, accK2, c2)) => (accT1 + accT2, accD1 + accD2, joinMap(accK1, accK2), c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accT, accD, accK, c) => (toKey(accK.maxBy(_._2)._1), toClass(accT/c, accD/c)) } //(pid, (k, cls))\n",
    "\n",
    "val job2 = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (_,_,_, e))) => (pid, if (e) 1 else 0) }.\n",
    "    join(playlistWithFollowers). // (pid, (e, num_followers))\n",
    "    aggregateByKey((0, 0, 0))(\n",
    "        { case ((accE, nF, c), (e, f)) => (accE + e, f, c + 1)  },\n",
    "        { case ((accE1, nF, c1), (accE2, _, c2)) => (accE1 + accE2, nF, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, nF, c) => (accE/c, nF) }. // (pid, avgE, NF)\n",
    "    join(playlistClasses). // (pid ((avgE, NF), (k, cls)))\n",
    "    map { case (pid, ((avgE, nF), (k, cls))) => ((k, cls), (avgE, nF))}.\n",
    "    aggregateByKey((0.0, 0, 0))(\n",
    "        { case ((accE, accF, c), (e, f)) => (accE + e, accF + f, c+1) },\n",
    "        { case ((accE1, accF1, c1), (accE2, accF2, c2)) => (accE1 + accE2, accF1 + accF2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, accF, c) => (accE/c, accF/c, c) }\n",
    "    \n",
    "val result = job2.collect    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No reduction in time was registered as the job took still `22 minutes` to perform. We therefore tried to change the execution plan. In the previous job, when joining `tracksInPlaylistWithFeatures` with `playlistFollowers`, we performed the so called `Join & Aggregate`. So we tried to perform the `Aggregate and Join` to see if there was any benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute the class of each playlist\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).cache\n",
    "\n",
    "val playlistClasses = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (t, d, k, _))) => (pid, (t, d, k)) }.\n",
    "    aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0))(\n",
    "        { case ((accT, accD, accK, c), (t, d, k)) => (accT + t, accD + d, incrementKey(accK, k), c + 1) },\n",
    "        { case ( (accT1, accD1, accK1, c1), (accT2, accD2, accK2, c2)) => (accT1 + accT2, accD1 + accD2, joinMap(accK1, accK2), c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accT, accD, accK, c) => (toKey(accK.maxBy(_._2)._1), toClass(accT/c, accD/c)) } //(pid, (k, cls))\n",
    "\n",
    "val playlistsWithClasses = playlistWithFollowers.join(playlistClasses).mapValues { case (f, (k, cls)) => (k, cls, f) }\n",
    "\n",
    "val job3 = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (_,_,_, e))) => (pid, if (e) 1 else 0) }.\n",
    "    aggregateByKey((0.0, 0))(\n",
    "        { case ((accE, c), e) => (accE + e, c + 1)},\n",
    "        { case ((accE1, c1), (accE2, c2)) => (accE1 + accE2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, c) => accE / c }.\n",
    "    join(playlistsWithClasses). // (pid, (avgE, (k, cls, f)))\n",
    "    map { case (pid, (avgE, (k, cls, nF))) => ((k, cls), (avgE, nF))}.\n",
    "    aggregateByKey((0.0, 0, 0))(\n",
    "        { case ((accE, accF, c), (e, f)) => (accE + e, accF + f, c+1) },\n",
    "        { case ((accE1, accF1, c1), (accE2, accF2, c2)) => (accE1 + accE2, accF1 + accF2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, accF, c) => (accE/c, accF/c, c) }\n",
    "    \n",
    "val result = job3.collect    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing an *Aggregate and Join* the execution time increase of `2 minutes`. So ultimately we tried to compute the playlist classes and the requested averages in a single aggregation step before joining with `playlistWithFollowers` and aggregating to obtain the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).\n",
    "    map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithClasses = tracksInPlaylistWithFeatures.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (toKey(k.maxBy(_._2)._1), toClass(accT/c, accD/c), ec/c, c) }) // (pid, (k, class, avgE, c))\n",
    "\n",
    "val job4 = playlistWithFollowers.join(tracksInPlaylistWithClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) } // ((k, class), (avgF, avgE, avgTC, c))\n",
    "        \n",
    "val result = job4.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last job took in total `20 minutes`. Overall we obtain a `4 minutes` improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further optimizations\n",
    "\n",
    "Since the last job was in term of time consuption more efficient, it has been choosen for further optimizations.\n",
    "First of all, by looking at the job details on SparkUI it becomes evident that the stage that is more costsly is the join between the `tracksInPlaylist` with the `features`. This may be due to the fact that the `tracksInPlaylist` form a total of 277 block thus 277 tasks. This very likely results in a big scheduling overhead and to the generation of a lot of intermediate files due to the shuffling strategy. So we tried reducing the number of partitions. \n",
    "\n",
    "### Coalescing the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpersistRDDs()\n",
    "// val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(150)\n",
    "// val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(50)\n",
    "// val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(10)\n",
    "val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(6)\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).\n",
    "    map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithClasses = tracksInPlaylistWithFeatures.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (toKey(k.maxBy(_._2)._1), toClass(accT/c, accD/c), ec/c, c) }) // (pid, (k, class, avgE, c))\n",
    "\n",
    "val job5 = playlistWithFollowers.join(tracksInPlaylistWithClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) } // ((k, class), (avgF, avgE, avgTC, c))\n",
    "job5.collect        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the the result of the various executions:\n",
    "\n",
    "|N. of tasks |Join step shuffle data size|Exection time|\n",
    "|------------|---------------------------|-------------|\n",
    "|150|12.2 GB|17 min|\n",
    "|50|7.0 GB|13 min|\n",
    "|10|1704.8 MB|14 min|\n",
    "|6|---|8.6 min|\n",
    "|5|1013.7MB|9.3 min|\n",
    "\n",
    "So the best number of partitions seems to be `6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing a partition criteria\n",
    "\n",
    "Next we tried to enforce the same partion criteria on the rdd involved in the join. We tried with different number of partiotions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "//val p = new HashPartitioner(50)\n",
    "//val p = new HashPartitioner(10)\n",
    "//val p = new HashPartitioner(6)\n",
    "val p = new HashPartitioner(5)\n",
    "\n",
    "unpersistRDDs()\n",
    "\n",
    "val features = featureRdd.map(t => (t.uri, (t.tempo, t.danceability, t.key))).\n",
    "  join(trackRdd.map(t => (t.uri, t.explicit))).\n",
    "  map { case (uri, ((t, d, k), e)) => (uri, (t, d, k, e)) }.partitionBy(p)\n",
    "val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).partitionBy(p)\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).\n",
    "    map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithClasses = tracksInPlaylistWithFeatures.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (toKey(k.maxBy(_._2)._1), toClass(accT/c, accD/c), ec/c, c) }).partitionBy(p) // (pid, (k, class, avgE, c))\n",
    "\n",
    "val job4 = playlistWithFollowers.partitionBy(p).join(tracksInPlaylistWithClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) } // ((k, class), (avgF, avgE, avgTC, c))\n",
    "val result = job4.collect        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the the result of the various executions:\n",
    "\n",
    "|N. of tasks |Join step shuffle data size|Exection time|\n",
    "|------------|---------------------------|-------------|\n",
    "|50|7.0 GB|13 min|\n",
    "|10|1704.8 MB|12 min|\n",
    "|6|---|7.5 min|\n",
    "|5|1013.7 MB|7.5 min|\n",
    "\n",
    "So the best number of partitions seems to be `6 or 5`.\n",
    "\n",
    "So overall we obtain a speed-up of $ S=\\frac{20 min}{7.5 min} = 2.66 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
