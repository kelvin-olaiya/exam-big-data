{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Big Data Exam Report @ UniBo a.y. 2023/2024\n",
    "\n",
    "- Manuel Andruccioli\n",
    "- Kelvin Olaiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT RUN THIS CELL -- ONLY FOR TYPE CHECKER\n",
    "import org.apache.spark.SparkContext\n",
    "val sc = new SparkContext(\"local[*]\", \"BigDataExam\")\n",
    "val spark = org.apache.spark.sql.SparkSession.builder.appName(\"BigDataExam\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.applicationId\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:52.873694Z",
     "start_time": "2024-04-24T12:35:52.567372Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpersistRDD(): Unit = {\n",
    "  sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures and definitions\n",
    "\n",
    "### Utility function for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:53.579889Z",
     "start_time": "2024-04-24T12:35:52.874545Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCharIndexes(line: String, char: Char): Seq[Int] = line.zipWithIndex.filter(_._1 == char).map(_._2) \n",
    "def splitAt(s: String, indices: Seq[Int]): Seq[String] = indices match {\n",
    "  case h +: t => s.splitAt(h) match {\n",
    "    case (a, b) => a +: splitAt(b, t.map(_ - h))\n",
    "  }\n",
    "  case Nil => Seq(s)\n",
    "}\n",
    "def parseCSVLine(l: String): Seq[String] = {\n",
    "  val apices = getCharIndexes(l, '\"').grouped(2).map { case Seq(a, b) => (a, b) }.toSeq\n",
    "  val commas = getCharIndexes(l, ',').filter(i => !apices.exists { case (a, b) => a < i && i < b })\n",
    "  return splitAt(l, commas).map(_.dropWhile(s => s == ',' || s == ' ')).map(_.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:54.214440Z",
     "start_time": "2024-04-24T12:35:53.581907Z"
    }
   },
   "outputs": [],
   "source": [
    "case class Track(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  duration: Int,\n",
    "  explicit: Boolean,\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  album_uri: String,\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Tracks {\n",
    "  def fromCSVLine(line: String): Option[Track] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, duration, explicit, artists, available_markets, album_uri, popularity) =>\n",
    "        try {\n",
    "          Some(Track(uri, name, duration.toInt, explicit.toBoolean, artists, available_markets, album_uri, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Playlist(\n",
    "  pid: Int,\n",
    "  name: String,\n",
    "  num_follower: Int,\n",
    ")\n",
    "\n",
    "object Playlists {\n",
    "  def fromCSVLine(line: String): Option[Playlist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, name, num_follower) =>\n",
    "        try {\n",
    "            Some(Playlist(pid.toInt, name, num_follower.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class TrackInPlaylist(\n",
    "  pid: Int,\n",
    "  track_uri: String,\n",
    "  pos: Int,\n",
    ")\n",
    "\n",
    "object TrackInPlaylists {\n",
    "  def fromCSVLine(line: String): Option[TrackInPlaylist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, track_uri, pos) =>\n",
    "        try {\n",
    "          Some(TrackInPlaylist(pid.toInt, track_uri, pos.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Artist(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  followers: Int,\n",
    "  genres: String,             // List of genres, separated by |\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Artists {\n",
    "  def fromCSVLine(line: String): Option[Artist] =\n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, followers, genres, popularity) =>\n",
    "        try {\n",
    "          Some(Artist(uri, name, followers.toInt, genres, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Album(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  album_type: String,         // album, compilation, single.\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  release_year: String,\n",
    "  total_tracks: Int,\n",
    ")\n",
    "\n",
    "object Albums {\n",
    "  def fromCSVLine(line: String): Option[Album] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, album_type, artists, available_markets, release_year, total_tracks) =>\n",
    "        try {\n",
    "          Some(Album(uri, name, album_type, artists, available_markets, release_year, total_tracks.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Feature(\n",
    "  uri: String,\n",
    "  key: Int,\n",
    "  loudness: Double,\n",
    "  tempo: Double,\n",
    "  mode: Boolean,\n",
    "  danceability: Double,\n",
    "  valence: Double,\n",
    "  instrumentalness: Double,\n",
    "  liveness: Double,\n",
    "  acousticness: Double,\n",
    "  energy: Double,\n",
    "  speechiness: Double,\n",
    ")\n",
    "\n",
    "object Features {\n",
    "  def fromCSVLine(line: String): Option[Feature] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, key, loudness, tempo, mode, danceability, valence, instrumentalness, liveness, acousticness, energy, speechiness) =>\n",
    "        try {\n",
    "          Some(Feature(uri, key.toInt, loudness.toDouble, tempo.toDouble, mode.toInt == 1, danceability.toDouble, valence.toDouble, instrumentalness.toDouble, liveness.toDouble, acousticness.toDouble, energy.toDouble, speechiness.toDouble))\n",
    "        } catch {\n",
    "          case e: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:55.960439Z",
     "start_time": "2024-04-24T12:35:54.216170Z"
    }
   },
   "outputs": [],
   "source": [
    "val datasetPath = \"dataset/\"\n",
    "val outputPath = s\"${datasetPath}output/\"\n",
    "\n",
    "val albumRdd = sc.textFile(s\"${datasetPath}albums.csv\").flatMap(Albums.fromCSVLine)\n",
    "val artistRdd = sc.textFile(s\"${datasetPath}artists.csv\").flatMap(Artists.fromCSVLine)\n",
    "val featureRdd = sc.textFile(s\"${datasetPath}features.csv\").flatMap(Features.fromCSVLine)\n",
    "val playlistRdd = sc.textFile(s\"${datasetPath}playlists.csv\").flatMap(Playlists.fromCSVLine)\n",
    "val trackInPlaylistRdd = sc.textFile(s\"${datasetPath}tracks_in_playlists.csv\").flatMap(TrackInPlaylists.fromCSVLine)\n",
    "val trackRdd = sc.textFile(s\"${datasetPath}tracks.csv\").flatMap(Tracks.fromCSVLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val albumRddCached = albumRdd.cache()\n",
    "val artistRddCached = artistRdd.cache()\n",
    "val featureRddCached = featureRdd.cache()\n",
    "val playlistRddCached = playlistRdd.cache()\n",
    "val trackInPlaylistRddCached = trackInPlaylistRdd.cache()\n",
    "val trackRddCached = trackRdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Number of Albums: ${albumRddCached.count()}\")\n",
    "println(s\"Number of Artists: ${artistRddCached.count()}\")\n",
    "println(s\"Number of Track's Feature: ${featureRddCached.count()}\")\n",
    "println(s\"Number of Playlist: ${playlistRddCached.count()}\")\n",
    "println(s\"Number of Tracks add in Playlists: ${trackInPlaylistRddCached.count()}\")\n",
    "println(s\"Number of Tracks: ${trackRddCached.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Average number of tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"In Playlist: ${(trackInPlaylistRddCached.count().toDouble / playlistRddCached.count().toDouble).round}\")\n",
    "println(s\"In Album: ${(albumRddCached.map(_.total_tracks).sum() / albumRddCached.count().toDouble).round}\")\n",
    "println(s\"Per Artist (double counting on artist): ${\n",
    "  trackRddCached.flatMap(t => t.artists.split('|').map(a => (a, 1))).\n",
    "    reduceByKey(_ + _).\n",
    "    map { case (_, count) => count }.\n",
    "    mean().round}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Explicit vs Non-Explicit Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "trackRddCached.map(t => (t.explicit, 1)).\n",
    "  reduceByKey(_ + _).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}explicit_vs_non_explicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Explicit vs Non-Explicit Tracks](./img/explicit_vs_non_explicit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tracks per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumRddCached.map(a => a.release_year).\n",
    "  distinct().collect().sorted\n",
    "\n",
    "albumRddCached.map(a => (a.uri, a.release_year)).\n",
    "  join(trackRddCached.map(t => (t.album_uri, (t.uri, t.popularity)))).\n",
    "  map { case (albumUri, (releaseYear, (trackUri, popularity))) => (releaseYear, 1) }.\n",
    "  reduceByKey(_ + _).\n",
    "  coalesce(1).\n",
    "  sortByKey().\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}tracks_per_year.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Track per Year](./img/tracks_per_year.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Top\n",
    "\n",
    "#### Most followed Playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlistRddCached.map(p => (p.pid, p.name, p.num_follower)).\n",
    "  sortBy(_._3, ascending = false).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}playlist_with_followers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Top 10 playlist per followers](./img/top10_playlist_per_followers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Most popular Playlist\n",
    "\n",
    "The playlist popularity is calculated as the mean of the popularity of the tracks in the playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackInPlaylistRddCached.\n",
    "  map(t => (t.track_uri, t.pid)).\n",
    "  join(trackRddCached.map(t => (t.uri, t.popularity))).\n",
    "  map { case (_, (pid, popularity)) => (pid, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }.\n",
    "  join(playlistRddCached.map(p => (p.pid, p.name))).\n",
    "  map { case (pid, (popularity, name)) => (pid, name, popularity) }.\n",
    "  sortBy(_._3, ascending = false).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}playlist_with_popularity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Top playlist per popularity](./img/top_playlist_per_popularity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpersistRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Job 1\n",
    "\n",
    "Given the following metrics:\n",
    "\n",
    "  - track's popularity\n",
    "  - average popularity of tracks in year\n",
    "  - artist popularity (average if more artists are present in the track)\n",
    "\n",
    "Understand which metrics influence mostly the playlists, averaging the values of the tracks appearing in each.\n",
    "Also, aggregate the playlists on the previously calculated influence by averaging for the number of playlist followers.\n",
    "\n",
    "> **most influent**: the metric that has the highest average value for a playlist. \n",
    "\n",
    "The query let us answering the following question:\n",
    "a playlist influenced most by the popularity of the tracks has, on average, 500 followers. (Same for the other two metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Non optimized\n",
    "\n",
    "First of all, we need to calculate the `popularity of the year`.\n",
    "\n",
    "$$ \\texttt{Popularity of the year} = \n",
    "\\frac{\n",
    "  \\sum{\\texttt{popularity of tracks in that year}}\n",
    "}{\n",
    "  \\texttt{Number of tracks in that year}\n",
    "}\n",
    "$$\n",
    "\n",
    "Given the fact that the `release year` of a track is on the album, we need to join the track with the album to get it.\n",
    "After this first step, we can calculate the average popularity of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, we need to get the `artist's popularity of a track`.\n",
    "Given the fact that a track can have multiple artists, the `artist's popularity of a track` will be a mean of the `artist popularity` of the artists in the track.\n",
    "\n",
    "$$ \\texttt{Artist's popularity of a track} =\n",
    "\\frac{\n",
    "  \\sum{\\texttt{popularity of artists in the track}}\n",
    "}{\n",
    "  \\texttt{Number of artists in the track}\n",
    "}\n",
    "$$\n",
    "\n",
    "> **Note**: The `artists_uri` is a *denormalized* field in the track, so we need to split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// track -> artist's popularity\n",
    "val trackWithArtistPopularity = trackRdd.flatMap(t => t.artists.split('|').map(artistUri => (artistUri, t.uri))).\n",
    "  join(artistRdd.map(a => (a.uri, a.popularity))).\n",
    "  map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "At this point we can join all the data to get the `popularity`, `average popularity of the year` and `artist's popularity` of a track.\n",
    "\n",
    "$$\n",
    "\\texttt{track_uri} \\rightarrow (\\texttt{popularity}, \\texttt{average popularity of the year}, \\texttt{artist's popularity})\n",
    "$$\n",
    "\n",
    "After that, we can join the `track` with the `playlist` and perform the aggregation, achieving the average of the metrics for each playlist:\n",
    "\n",
    "$$\n",
    "\\texttt{pid} \\rightarrow (\\texttt{popularity}, \\texttt{average popularity of the year}, \\texttt{artist's popularity})\n",
    "$$\n",
    "\n",
    "Now, we can calculate the `most influent` metric for each playlist taking the maximum value of the metrics.\n",
    "  \n",
    "Finally, we can aggregate the playlists by the `most influent` metric and calculate the average number of followers for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val job = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }.\n",
    "  join(trackWithArtistPopularity).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(trackInPlaylistRdd.map(t => (t.track_uri, t.pid))).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear, avgArtistPopularity), pid)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(playlistRdd.map(p => (p.pid, p.num_follower))).\n",
    "  map { case (pid, ((popularity, avgPopularityInYear, avgArtistPopularity), numFollower)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity, numFollower)) }.\n",
    "  aggregateByKey((0.0, 0.0, 0.0, 0, 0))(\n",
    "    { case ((accPop, accAvgPopInYear, accAvgArtistPop, _, count), (popularity, avgPopularityInYear, avgArtistPopularity, followers)) =>\n",
    "      (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, followers, count + 1)\n",
    "    },\n",
    "    { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, followers, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, _, count2)) =>\n",
    "      (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, followers, count1 + count2)\n",
    "    }\n",
    "  ).\n",
    "  mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, followers, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count, followers) }.\n",
    "  map {\n",
    "    case (pid, (avgPop, avgPopInYear, avgArtistPop, followers)) =>\n",
    "      val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "      val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "      (indexOfBestAvg, followers)\n",
    "  }.\n",
    "  map { case (indexOfBestAvg, followers) => (indexOfBestAvg, (followers, 1)) }.\n",
    "  reduceByKey { case ((followers1, c1), (followers2, c2)) => (followers1 + followers2, c1 + c2) }.\n",
    "  mapValues { case (accF, c) => accF / c }\n",
    "\n",
    "spark.time {\n",
    "  job.collect.foreach {\n",
    "    case (0, avgFollowers) => println(s\"Playlist influenced most by Track's popularity has $avgFollowers followers on average\")\n",
    "    case (1, avgFollowers) => println(s\"Playlist influenced most by popularity of the year has $avgFollowers followers on average\")\n",
    "    case (2, avgFollowers) => println(s\"Playlist influenced most by Artist's popularity has $avgFollowers followers on average\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Optimization\n",
    "\n",
    "The first part of the optimization process will be performed evaluating multiple different execution plan on a restricted part of the entire job.\n",
    "The best one, according to the execution time, will be chosen and added to the final optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Self-join\n",
    "\n",
    "The first part of the job involves a self-join, and it could be resumed as the following steps:\n",
    "\n",
    "1. Join tracks on album (only to get the year of the track)\n",
    "2. Aggregate popularity by year to get the `popularity per year`\n",
    "3. Join track with their popularity per year\n",
    "\n",
    "Only for conceptual evaluation, it's been taken the following part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, t.popularity)).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, (popularity, releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin1 = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin1.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Broadcast variable\n",
    "\n",
    "Considering the small size of `Album` table (`289.6 MiB`, `20.1 MiB` after the map), the could be done using a *broadcast variable* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val albumsBroadcast = sc.broadcast(albumRdd.map(a => (a.uri, a.release_year)).collectAsMap())\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.flatMap(t => albumsBroadcast.value.get(t.album_uri).map((_, t.popularity))).\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin2 = trackRdd.flatMap(t => albumsBroadcast.value.get(t.album_uri).map((_, (t.uri, t.popularity)))).\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin2.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### RDD caching\n",
    "\n",
    "According to the previous consideration, the sub job's been evaluated also using the RDD caching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val albumRddCached1 = albumRdd.map(a => (a.uri, a.release_year)).cache()\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, t.popularity)).\n",
    "  join(albumRddCached1).\n",
    "  map { case (albumUri, (popularity, releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin3 = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRddCached1).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin3.count()\n",
    "}\n",
    "unpersistRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The optimization could be extended also to the `Track` table, considering it's size (`982.8 MiB`, `120.7 MiB` after the map).\n",
    "\n",
    "> **Note**: the `Track` table should be persisted considering all the common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trackRddCached2 = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).cache()\n",
    "val albumRddCached2 = albumRdd.map(a => (a.uri, a.release_year)).cache()\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRddCached2.\n",
    "  join(albumRddCached2).\n",
    "  map { case (albumUri, ((_, popularity), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin4 = trackRddCached2.\n",
    "  join(albumRddCached2).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin4.count()\n",
    "}\n",
    "unpersistRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aggregate before join & Hash partitioning of biggest RDD\n",
    "\n",
    "##### Aggregate before join\n",
    "\n",
    "Considering the initial job, the join on `Playlist` table (to get the number of followers) could be done after the first aggregation over pid on `Track_in_Playlist` table.\n",
    "\n",
    "Usually, aggregate before join is a **good practice**, in order to avoid to shuffle more data than needed.\n",
    "But, in this particular case, produce worse performance as we can see in the image below.\n",
    "\n",
    "In particular:\n",
    "- **Original Job**: `~9.7 min` image 1\n",
    "- **Aggregate before join**: `~13 min` image 2\n",
    "\n",
    "[Original Job Spark UI](./img/original_job.png)\n",
    "\n",
    "[Aggregate before join Spark UI](./img/aggregate_before_join.png)\n",
    "\n",
    "##### Hash partitioning of biggest RDD\n",
    "\n",
    "It's been tried also a partitioning of the biggest RDD (`Track_in_Playlist`, `8.6 GiB` input, `~193M` of records) in order to avoid a *reshuffle* during the join part.\n",
    "It's been taken `6` as number of partitions, considering the number of executors (2) and the number of cores (3) per executor.\n",
    "\n",
    "The result is similar to the previous one, producing a worse performance than the original job (`~11 min`).\n",
    "\n",
    "##### Merging the two optimizations\n",
    "\n",
    "The two optimizations described above have been merged and the job got a performance improvement: now the aggregation is done before the join and the biggest RDD is shuffled, avoiding the *reshuffle* during the join.\n",
    "\n",
    "The job reach an execution time of `~7.9 min`.\n",
    "\n",
    "[Aggregate before join & Hash partitioning Spark UI](./img/aggregate_before_join_and_partitioner.png)\n",
    "\n",
    "The execution has been tested with other number of partitions, but the best result has been reached with `6` partitions.\n",
    "\n",
    "- **12 partition**: `~8.3 min`\n",
    "- **18 partition**: `~8.9 min`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "// track -> artist's popularity\n",
    "val trackWithArtistPopularity = trackRdd.flatMap(t => t.artists.split('|').map(artistUri => (artistUri, t.uri))).\n",
    "  join(artistRdd.map(a => (a.uri, a.popularity))).\n",
    "  map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "import org.apache.spark.HashPartitioner\n",
    "val partitioner = new HashPartitioner(6)\n",
    "\n",
    "val jobAggregationBeforeJoinAndPartitioner = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }.\n",
    "  join(trackWithArtistPopularity).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).partitionBy(partitioner)). // here the partitioner\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear, avgArtistPopularity), pid)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  // the join from here is pushed down to (1)\n",
    "  aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "    { case ((accPop, accAvgPopInYear, accAvgArtistPop, count), (popularity, avgPopularityInYear, avgArtistPopularity)) =>\n",
    "      (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, count + 1)\n",
    "    },\n",
    "    { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, count2)) =>\n",
    "      (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, count1 + count2)\n",
    "    }\n",
    "  ).\n",
    "  mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count) }.\n",
    "  map {\n",
    "    case (pid, (avgPop, avgPopInYear, avgArtistPop)) =>\n",
    "      val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "      val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "      (pid, indexOfBestAvg)\n",
    "  }.\n",
    "  // (1) here\n",
    "  join(playlistRdd.map(p => (p.pid, p.num_follower))).\n",
    "  map { case (pid, (indexOfBestAvg, numFollower)) => (indexOfBestAvg, (numFollower, 1)) }.\n",
    "  reduceByKey { case ((accFollowers1, c1), (accFollowers2, c2)) => (accFollowers1 + accFollowers2, c1 + c2) }.\n",
    "  mapValues { case (accF, c) => accF / c }\n",
    "\n",
    "spark.time {\n",
    "  jobAggregationBeforeJoinAndPartitioner.collect.foreach {\n",
    "    case (0, avgFollowers) => println(s\"Playlist influenced most by Track's popularity has $avgFollowers followers on average\")\n",
    "    case (1, avgFollowers) => println(s\"Playlist influenced most by popularity of the year has $avgFollowers followers on average\")\n",
    "    case (2, avgFollowers) => println(s\"Playlist influenced most by Artist's popularity has $avgFollowers followers on average\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all the optimizations\n",
    "\n",
    "The final job has been executed merging the optimizations described above.\n",
    "\n",
    "- **RDD caching**: `Album` and `Track`\n",
    "- **Aggregate before join**\n",
    "- **Hash partitioning of biggest RDD**\n",
    "\n",
    "The job has been executed in `~7.3 min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpersistRDD()\n",
    "\n",
    "val finalTrackRddCached = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity, t.artists))).cache()\n",
    "val finalAlbumRddCached = albumRdd.map(a => (a.uri, a.release_year)).cache()\n",
    "\n",
    "val avgPopPerYear = finalTrackRddCached.\n",
    "  join(finalAlbumRddCached).\n",
    "  map { case (albumUri, ((trackUri, popularity, _), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count } // year -> avg popularity of tracks in that year\n",
    "\n",
    "val trackWithArtistPopularity = finalTrackRddCached.flatMap(t => t._2._3.split('|').map(artistUri => (artistUri, t._2._1))).\n",
    "  join(artistRdd.map(a => (a.uri, a.popularity))).\n",
    "  map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count } // track -> avg popularity of artists of that track\n",
    "\n",
    "import org.apache.spark.HashPartitioner\n",
    " \n",
    "val finalJob = finalTrackRddCached.map(t => (t._1, (t._2._1, t._2._2))).\n",
    "  join(finalAlbumRddCached).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(avgPopPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }.\n",
    "  join(trackWithArtistPopularity).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).partitionBy(new HashPartitioner(6))).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear, avgArtistPopularity), pid)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity))}.\n",
    "  aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "    { case ((accPop, accAvgPopInYear, accAvgArtistPop, count), (popularity, avgPopularityInYear, avgArtistPopularity)) =>\n",
    "      (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, count + 1)\n",
    "    },\n",
    "    { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, count2)) =>\n",
    "      (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, count1 + count2)\n",
    "    }\n",
    "  ).\n",
    "  mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count) }.\n",
    "  map {\n",
    "    case (pid, (avgPop, avgPopInYear, avgArtistPop)) =>\n",
    "      val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "      val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "      (pid, indexOfBestAvg)\n",
    "  }.\n",
    "  join(playlistRdd.map(p => (p.pid, p.num_follower))).\n",
    "  map { case (pid, (indexOfBestAvg, numFollower)) => (indexOfBestAvg, (numFollower, 1)) }.\n",
    "  reduceByKey { case ((accFollowers1, c1), (accFollowers2, c2)) => (accFollowers1 + accFollowers2, c1 + c2) }.\n",
    "  mapValues { case (accF, c) => accF / c }\n",
    "\n",
    "spark.time {\n",
    "  finalJob.collect.foreach {\n",
    "    case (0, avgFollowers) => println(s\"Playlist influenced most by Track's popularity has $avgFollowers followers on average\")\n",
    "    case (1, avgFollowers) => println(s\"Playlist influenced most by popularity of the year has $avgFollowers followers on average\")\n",
    "    case (2, avgFollowers) => println(s\"Playlist influenced most by Artist's popularity has $avgFollowers followers on average\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![Job 1 result](./img/job1_results.png)\n",
    "\n",
    "Other tests have been done, such as:\n",
    "\n",
    "- repartitioning and coalescing of `Tracks`\n",
    "- repartitioning and coalescing after aggregation during the job part\n",
    "- unpersisting the RDDs when they are not needed anymore\n",
    "\n",
    "But the best performance has been reached with the optimizations described above.\n",
    "\n",
    "Here the resume of the execution time:\n",
    "\n",
    "| Job                                           | Execution Time |\n",
    "|-----------------------------------------------|----------------|\n",
    "| Original Job                                  | 9.7 min        |\n",
    "| Aggregate before join                         | 13 min         |\n",
    "| Hash partitioning (6)                         | 11 min         |\n",
    "| Aggregate before join + Hash partitioning (6) | 7.9 min        |\n",
    "| Final Job                                     | 7.3 min        |\n",
    "| Final Job with repartition (300) on Tracks    | 7.7 min        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 2\n",
    "\n",
    "Given the following classes: slowly danceable (tempo <= 130BPM, danceability > 0.5), swiftly danceable (tempo >130BPM, danceability > 0.5), slowly undanceable (tempo <= 130BPM, danceability <= 0.5), swiftly undanceable (tempo >130BPM, danceability <= 0.5); and the various keys (C, C#/Db, ...).\n",
    "  for each class and (key ---OR--- range of followers) get:\n",
    "  - The number of playlist.\n",
    "  - Average playlist's explicitness percentage.\n",
    "  - Average number of tracks in playlist.\n",
    "  - Average number of playlist followers.\n",
    "  <!-- - Average playlist danceability.\n",
    "  - Average playlist tempo. -->\n",
    "  (The key of a playlist is the most present key among its tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toClass(tempo: Double, danceablility: Double): String = (tempo, danceablility) match {\n",
    "  case (t, d) if t <= 130 && d > 0.5 => \"slowly danceable\"\n",
    "  case (t, d) if t > 130 && d > 0.5 => \"swiftly danceable\"\n",
    "  case (t, d) if t <= 130 && d <= 0.5 => \"slowly undanceable\"\n",
    "  case (t, d) if t > 130 && d <= 0.5 => \"swiftly undanceable\"\n",
    "}\n",
    "\n",
    "def joinMap(map1: Map[Int, Int], map2: Map[Int, Int]): Map[Int, Int] = map1.map { case(k, v) => (k, map2.getOrElse(k, 0) + v) }\n",
    "def incrementOnKey(map: Map[Int, Int], key: Int) = {\n",
    "  val current = map.getOrElse(key, 0)\n",
    "  map.updated(key, current + 1)\n",
    "}\n",
    "\n",
    "val features = featureRdd.map(t => (t.uri, (t.tempo, t.danceability, t.key))).\n",
    "  join(trackRdd.map(t => (t.uri, t.explicit))).\n",
    "  map { case (uri, ((t, d, k), e)) => (uri, (t, d, k, e)) }\n",
    "\n",
    "val trackInPlaylistWithFeatures = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).join(features)\n",
    "\n",
    "val playlistClasses = trackInPlaylistWithFeatures.\n",
    "        map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementOnKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (accT/c, accD/c, k.maxBy(_._2)._1, ec/c, c) }).\n",
    "        map { case (pid, (avgT, avgD, k, avgE, c)) => (pid, (k, toClass(avgT, avgD), avgE, c)) }// (pid, (k, class, avgE, c))\n",
    "\n",
    "playlistRdd.map(p => (p.pid, p.num_follower)).join(playlistClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) }. // ((k, class), (avgF, avgE, avgTC, c))\n",
    "        collect.foreach(println)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
