{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Big Data Exam Report @ UniBo a.y. 2023/2024\n",
    "\n",
    "- Manuel Andruccioli\n",
    "- Kelvin Olaiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT RUN THIS CELL -- ONLY FOR TYPE CHECKER\n",
    "import org.apache.spark.SparkContext\n",
    "val sc = new SparkContext(\"local[*]\", \"BigDataExam\")\n",
    "val spark = org.apache.spark.sql.SparkSession.builder.appName(\"BigDataExam\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.applicationId\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:52.873694Z",
     "start_time": "2024-04-24T12:35:52.567372Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpersistRDD(): Unit = {\n",
    "  sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures and definitions\n",
    "\n",
    "### Utility function for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:53.579889Z",
     "start_time": "2024-04-24T12:35:52.874545Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCharIndexes(line: String, char: Char): Seq[Int] = line.zipWithIndex.filter(_._1 == char).map(_._2) \n",
    "def splitAt(s: String, indices: Seq[Int]): Seq[String] = indices match {\n",
    "  case h +: t => s.splitAt(h) match {\n",
    "    case (a, b) => a +: splitAt(b, t.map(_ - h))\n",
    "  }\n",
    "  case Nil => Seq(s)\n",
    "}\n",
    "def parseCSVLine(l: String): Seq[String] = {\n",
    "  val apices = getCharIndexes(l, '\"').grouped(2).map { case Seq(a, b) => (a, b) }.toSeq\n",
    "  val commas = getCharIndexes(l, ',').filter(i => !apices.exists { case (a, b) => a < i && i < b })\n",
    "  return splitAt(l, commas).map(_.dropWhile(s => s == ',' || s == ' ')).map(_.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:54.214440Z",
     "start_time": "2024-04-24T12:35:53.581907Z"
    }
   },
   "outputs": [],
   "source": [
    "case class Track(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  duration: Int,\n",
    "  explicit: Boolean,\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  album_uri: String,\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Tracks {\n",
    "  def fromCSVLine(line: String): Option[Track] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, duration, explicit, artists, available_markets, album_uri, popularity) =>\n",
    "        try {\n",
    "          Some(Track(uri, name, duration.toInt, explicit.toBoolean, artists, available_markets, album_uri, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Playlist(\n",
    "  pid: Int,\n",
    "  name: String,\n",
    "  num_follower: Int,\n",
    ")\n",
    "\n",
    "object Playlists {\n",
    "  def fromCSVLine(line: String): Option[Playlist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, name, num_follower) =>\n",
    "        try {\n",
    "            Some(Playlist(pid.toInt, name, num_follower.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class TrackInPlaylist(\n",
    "  pid: Int,\n",
    "  track_uri: String,\n",
    "  pos: Int,\n",
    ")\n",
    "\n",
    "object TrackInPlaylists {\n",
    "  def fromCSVLine(line: String): Option[TrackInPlaylist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, track_uri, pos) =>\n",
    "        try {\n",
    "          Some(TrackInPlaylist(pid.toInt, track_uri, pos.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Artist(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  followers: Int,\n",
    "  genres: String,             // List of genres, separated by |\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Artists {\n",
    "  def fromCSVLine(line: String): Option[Artist] =\n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, followers, genres, popularity) =>\n",
    "        try {\n",
    "          Some(Artist(uri, name, followers.toInt, genres, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Album(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  album_type: String,         // album, compilation, single.\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  release_year: String,\n",
    "  total_tracks: Int,\n",
    ")\n",
    "\n",
    "object Albums {\n",
    "  def fromCSVLine(line: String): Option[Album] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, album_type, artists, available_markets, release_year, total_tracks) =>\n",
    "        try {\n",
    "          Some(Album(uri, name, album_type, artists, available_markets, release_year, total_tracks.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Feature(\n",
    "  uri: String,\n",
    "  key: Int,\n",
    "  loudness: Double,\n",
    "  tempo: Double,\n",
    "  mode: Boolean,\n",
    "  danceability: Double,\n",
    "  valence: Double,\n",
    "  instrumentalness: Double,\n",
    "  liveness: Double,\n",
    "  acousticness: Double,\n",
    "  energy: Double,\n",
    "  speechiness: Double,\n",
    ")\n",
    "\n",
    "object Features {\n",
    "  def fromCSVLine(line: String): Option[Feature] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, key, loudness, tempo, mode, danceability, valence, instrumentalness, liveness, acousticness, energy, speechiness) =>\n",
    "        try {\n",
    "          Some(Feature(uri, key.toInt, loudness.toDouble, tempo.toDouble, mode.toInt == 1, danceability.toDouble, valence.toDouble, instrumentalness.toDouble, liveness.toDouble, acousticness.toDouble, energy.toDouble, speechiness.toDouble))\n",
    "        } catch {\n",
    "          case e: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T12:35:55.960439Z",
     "start_time": "2024-04-24T12:35:54.216170Z"
    }
   },
   "outputs": [],
   "source": [
    "val datasetPath = \"dataset/\"\n",
    "val outputPath = s\"${datasetPath}output/\"\n",
    "\n",
    "val albumRdd = sc.textFile(s\"${datasetPath}albums.csv\").flatMap(Albums.fromCSVLine)\n",
    "val artistRdd = sc.textFile(s\"${datasetPath}artists.csv\").flatMap(Artists.fromCSVLine)\n",
    "val featureRdd = sc.textFile(s\"${datasetPath}features.csv\").flatMap(Features.fromCSVLine)\n",
    "val playlistRdd = sc.textFile(s\"${datasetPath}playlists.csv\").flatMap(Playlists.fromCSVLine)\n",
    "val trackInPlaylistRdd = sc.textFile(s\"${datasetPath}tracks_in_playlists.csv\").flatMap(TrackInPlaylists.fromCSVLine)\n",
    "val trackRdd = sc.textFile(s\"${datasetPath}tracks.csv\").flatMap(Tracks.fromCSVLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val albumRddCached = albumRdd.cache()\n",
    "val artistRddCached = artistRdd.cache()\n",
    "val featureRddCached = featureRdd.cache()\n",
    "val playlistRddCached = playlistRdd.cache()\n",
    "val trackInPlaylistRddCached = trackInPlaylistRdd.cache()\n",
    "val trackRddCached = trackRdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Number of Albums: ${albumRddCached.count()}\")\n",
    "println(s\"Number of Artists: ${artistRddCached.count()}\")\n",
    "println(s\"Number of Track's Feature: ${featureRddCached.count()}\")\n",
    "println(s\"Number of Playlist: ${playlistRddCached.count()}\")\n",
    "println(s\"Number of Tracks add in Playlists: ${trackInPlaylistRddCached.count()}\")\n",
    "println(s\"Number of Tracks: ${trackRddCached.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Average number of tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"In Playlist: ${(trackInPlaylistRddCached.count().toDouble / playlistRddCached.count().toDouble).round}\")\n",
    "println(s\"In Album: ${(albumRddCached.map(_.total_tracks).sum() / albumRddCached.count().toDouble).round}\")\n",
    "println(s\"Per Artist (double counting on artist): ${\n",
    "  trackRddCached.flatMap(t => t.artists.split('|').map(a => (a, 1))).\n",
    "    reduceByKey(_ + _).\n",
    "    map { case (_, count) => count }.\n",
    "    mean().round}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Explicit vs Non-Explicit Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "trackRddCached.map(t => (t.explicit, 1)).\n",
    "  reduceByKey(_ + _).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}explicit_vs_non_explicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Explicit vs Non-Explicit Tracks](./img/explicit_vs_non_explicit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tracks per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumRddCached.map(a => a.release_year).\n",
    "  distinct().collect().sorted\n",
    "\n",
    "albumRddCached.map(a => (a.uri, a.release_year)).\n",
    "  join(trackRddCached.map(t => (t.album_uri, (t.uri, t.popularity)))).\n",
    "  map { case (albumUri, (releaseYear, (trackUri, popularity))) => (releaseYear, 1) }.\n",
    "  reduceByKey(_ + _).\n",
    "  coalesce(1).\n",
    "  sortByKey().\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}tracks_per_year.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Track per Year](./img/tracks_per_year.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Top\n",
    "\n",
    "#### Most followed Playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlistRddCached.map(p => (p.pid, p.name, p.num_follower)).\n",
    "  sortBy(_._3, ascending = false).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}playlist_with_followers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Top 10 playlist per followers](./img/top10_playlist_per_followers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Most popular Playlist\n",
    "\n",
    "The playlist popularity is calculated as the mean of the popularity of the tracks in the playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackInPlaylistRddCached.\n",
    "  map(t => (t.track_uri, t.pid)).\n",
    "  join(trackRddCached.map(t => (t.uri, t.popularity))).\n",
    "  map { case (_, (pid, popularity)) => (pid, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }.\n",
    "  join(playlistRddCached.map(p => (p.pid, p.name))).\n",
    "  map { case (pid, (popularity, name)) => (pid, name, popularity) }.\n",
    "  sortBy(_._3, ascending = false).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}playlist_with_popularity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Top playlist per popularity](./img/top_playlist_per_popularity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpersistRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Job 1\n",
    "\n",
    "Given the following metrics:\n",
    "\n",
    "  - track's popularity\n",
    "  - average popularity of tracks in year\n",
    "  - artist popularity (average if more artists are present in the track)\n",
    "\n",
    "Understand which metrics influence mostly the playlists, averaging the values of the tracks appearing in each.\n",
    "Also, aggregate the playlists on the previously calculated influence by averaging for the number of playlist followers.\n",
    "\n",
    "> **most influent**: the metric that has the highest average value for a playlist. \n",
    "\n",
    "The query let us answering the following question:\n",
    "a playlist influenced most by the popularity of the tracks has, on average, 500 followers. (Same for the other two metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Non optimized\n",
    "\n",
    "First of all, we need to calculate the `popularity of the year`.\n",
    "\n",
    "$$ \\texttt{Popularity of the year} = \n",
    "\\frac{\n",
    "  \\sum{\\texttt{popularity of tracks in that year}}\n",
    "}{\n",
    "  \\texttt{Number of tracks in that year}\n",
    "}\n",
    "$$\n",
    "\n",
    "Given the fact that the `release year` of a track is on the album, we need to join the track with the album to get it.\n",
    "After this first step, we can calculate the average popularity of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, we need to get the `artist's popularity of a track`.\n",
    "Given the fact that a track can have multiple artists, the `artist's popularity of a track` will be a mean of the `artist popularity` of the artists in the track.\n",
    "\n",
    "$$ \\texttt{Artist's popularity of a track} =\n",
    "\\frac{\n",
    "  \\sum{\\texttt{popularity of artists in the track}}\n",
    "}{\n",
    "  \\texttt{Number of artists in the track}\n",
    "}\n",
    "$$\n",
    "\n",
    "> **Note**: The `artists_uri` is a *denormalized* field in the track, so we need to split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// track -> artist's popularity\n",
    "val trackWithArtistPopularity = trackRdd.flatMap(t => t.artists.split('|').map(artistUri => (artistUri, t.uri))).\n",
    "  join(artistRdd.map(a => (a.uri, a.popularity))).\n",
    "  map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "At this point we can join all the data to get the `popularity`, `average popularity of the year` and `artist's popularity` of a track.\n",
    "\n",
    "$$\n",
    "\\texttt{track_uri} \\rightarrow (\\texttt{popularity}, \\texttt{average popularity of the year}, \\texttt{artist's popularity})\n",
    "$$\n",
    "\n",
    "After that, we can join the `track` with the `playlist` and perform the aggregation, achieving the average of the metrics for each playlist:\n",
    "\n",
    "$$\n",
    "\\texttt{pid} \\rightarrow (\\texttt{popularity}, \\texttt{average popularity of the year}, \\texttt{artist's popularity})\n",
    "$$\n",
    "\n",
    "Now, we can calculate the `most influent` metric for each playlist taking the maximum value of the metrics.\n",
    "  \n",
    "Finally, we can aggregate the playlists by the `most influent` metric and calculate the average number of followers for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val job = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }.\n",
    "  join(trackWithArtistPopularity).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(trackInPlaylistRdd.map(t => (t.track_uri, t.pid))).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear, avgArtistPopularity), pid)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(playlistRdd.map(p => (p.pid, p.num_follower))).\n",
    "  map { case (pid, ((popularity, avgPopularityInYear, avgArtistPopularity), numFollower)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity, numFollower)) }.\n",
    "  aggregateByKey((0.0, 0.0, 0.0, 0, 0))(\n",
    "    { case ((accPop, accAvgPopInYear, accAvgArtistPop, _, count), (popularity, avgPopularityInYear, avgArtistPopularity, followers)) =>\n",
    "      (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, followers, count + 1)\n",
    "    },\n",
    "    { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, followers, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, _, count2)) =>\n",
    "      (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, followers, count1 + count2)\n",
    "    }\n",
    "  ).\n",
    "  mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, followers, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count, followers) }.\n",
    "  map {\n",
    "    case (pid, (avgPop, avgPopInYear, avgArtistPop, followers)) =>\n",
    "      val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "      val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "      (indexOfBestAvg, followers)\n",
    "  }.\n",
    "  map { case (indexOfBestAvg, followers) => (indexOfBestAvg, (followers, 1)) }.\n",
    "  reduceByKey { case ((followers1, c1), (followers2, c2)) => (followers1 + followers2, c1 + c2) }.\n",
    "  mapValues { case (accF, c) => accF / c }\n",
    "\n",
    "spark.time {\n",
    "  job.collect.foreach {\n",
    "    case (0, avgFollowers) => println(s\"Playlist influenced most by Track's popularity has $avgFollowers followers on average\")\n",
    "    case (1, avgFollowers) => println(s\"Playlist influenced most by popularity of the year has $avgFollowers followers on average\")\n",
    "    case (2, avgFollowers) => println(s\"Playlist influenced most by Artist's popularity has $avgFollowers followers on average\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Optimization\n",
    "\n",
    "The first part of the optimization process will be performed evaluating multiple different execution plan on a restricted part of the entire job.\n",
    "The best one, according to the execution time, will be chosen and added to the final optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Self-join\n",
    "\n",
    "The first part of the job involves a self-join, and it could be resumed as the following steps:\n",
    "\n",
    "1. Join tracks on album (only to get the year of the track)\n",
    "2. Aggregate popularity by year to get the `popularity per year`\n",
    "3. Join track with their popularity per year\n",
    "\n",
    "Only for conceptual evaluation, it's been taken the following part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, t.popularity)).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, (popularity, releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin1 = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin1.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Broadcast variable\n",
    "\n",
    "Considering the small size of `Album` table (`289.6 MiB`, `20.1 MiB` after the map), the could be done using a *broadcast variable* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val albumsBroadcast = sc.broadcast(albumRdd.map(a => (a.uri, a.release_year)).collectAsMap())\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.flatMap(t => albumsBroadcast.value.get(t.album_uri).map((_, t.popularity))).\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin2 = trackRdd.flatMap(t => albumsBroadcast.value.get(t.album_uri).map((_, (t.uri, t.popularity)))).\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin2.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### RDD caching\n",
    "\n",
    "According to the previous consideration, the sub job's been evaluated also using the RDD caching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val albumRddCached = albumRdd.map(a => (a.uri, a.release_year)).cache()\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, t.popularity)).\n",
    "  join(albumRddCached).\n",
    "  map { case (albumUri, (popularity, releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin3 = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRddCached).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin3.count()\n",
    "}\n",
    "unpersistRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The optimization could be extended also to the `Track` table, considering it's size (`982.8 MiB`, `120.7 MiB` after the map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trackRddCached = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).cache()\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRddCached.\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((_, popularity), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin4 = trackRddCached.\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin4.count()\n",
    "}\n",
    "unpersistRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can consider to cache both `Track` and `Album` RDDs, given them sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trackRddCached = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).cache()\n",
    "val albumRddCached = albumRdd.map(a => (a.uri, a.release_year)).cache()\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRddCached.\n",
    "  join(albumRddCached).\n",
    "  map { case (albumUri, ((_, popularity), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "val selfJoin5 = trackRddCached.\n",
    "  join(albumRddCached).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }\n",
    "\n",
    "spark.time {\n",
    "  selfJoin5.count()\n",
    "}\n",
    "unpersistRDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "| Optimization            | Execution Time |\n",
    "|-------------------------|----------------|\n",
    "| Original                | 50 sec         |\n",
    "| Broadcast               | 36 sec         |\n",
    "| Album Caching           | 46 sec         |\n",
    "| Track Caching           | 35 sec         |\n",
    "| Album and Track Caching | 32 sec         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aggregate before join & Hash partitioning of biggest RDD\n",
    "\n",
    "##### Aggregate before join\n",
    "\n",
    "Considering the initial job, the join on `Playlist` table (to get the number of followers) could be done after the first aggregation over pid on `Track_in_Playlist` table.\n",
    "\n",
    "Usually, aggregate before join is a **good practice**, in order to avoid to shuffle more data than needed.\n",
    "But, in this particular case, produce worse performance as we can see in the image below.\n",
    "\n",
    "In particular:\n",
    "- **Original Job**: `~9.7 min` image 1\n",
    "- **Aggregate before join**: `~13 min` image 2\n",
    "\n",
    "[Original Job Spark UI](./img/original_job.png)\n",
    "\n",
    "[Aggregate before join Spark UI](./img/aggregate_before_join.png)\n",
    "\n",
    "##### Hash partitioning of biggest RDD\n",
    "\n",
    "It's been tried also a partitioning of the biggest RDD (`Track_in_Playlist`, `8.6 GiB` input, `~193M` of records) in order to avoid a *reshuffle* during the join part.\n",
    "It's been taken `6` as number of partitions, considering the number of executors (2) and the number of cores (3) per executor.\n",
    "\n",
    "The result is similar to the previous one, producing a worse performance than the original job (`~11 min`).\n",
    "\n",
    "##### Merging the two optimizations\n",
    "\n",
    "The two optimizations described above have been merged and the job got a performance improvement: now the aggregation is done before the join and the biggest RDD is shuffled, avoiding the *reshuffle* during the join.\n",
    "\n",
    "The job reach an execution time of `~7.9 min`.\n",
    "\n",
    "[Aggregate before join & Hash partitioning Spark UI](./img/aggregate_before_join_and_partitioner.png)\n",
    "\n",
    "The execution has been tested with other number of partitions, but the best result has been reached with `6` partitions.\n",
    "\n",
    "- **12 partition**: `~8.3 min`\n",
    "- **18 partition**: `~8.9 min`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// year -> popularity of the year\n",
    "val popularityPerYear = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "// track -> artist's popularity\n",
    "val trackWithArtistPopularity = trackRdd.flatMap(t => t.artists.split('|').map(artistUri => (artistUri, t.uri))).\n",
    "  join(artistRdd.map(a => (a.uri, a.popularity))).\n",
    "  map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }\n",
    "\n",
    "import org.apache.spark.HashPartitioner\n",
    "val partitioner = new HashPartitioner(6)\n",
    "\n",
    "val jobAggregationBeforeJoinAndPartitioner = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity))).\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popularityPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }.\n",
    "  join(trackWithArtistPopularity).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).partitionBy(partitioner)). // here the partitioner\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear, avgArtistPopularity), pid)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  // the join from here is pushed down to (1)\n",
    "  aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "    { case ((accPop, accAvgPopInYear, accAvgArtistPop, count), (popularity, avgPopularityInYear, avgArtistPopularity)) =>\n",
    "      (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, count + 1)\n",
    "    },\n",
    "    { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, count2)) =>\n",
    "      (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, count1 + count2)\n",
    "    }\n",
    "  ).\n",
    "  mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count) }.\n",
    "  map {\n",
    "    case (pid, (avgPop, avgPopInYear, avgArtistPop)) =>\n",
    "      val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "      val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "      (pid, indexOfBestAvg)\n",
    "  }.\n",
    "  // (1) here\n",
    "  join(playlistRdd.map(p => (p.pid, p.num_follower))).\n",
    "  map { case (pid, (indexOfBestAvg, numFollower)) => (indexOfBestAvg, (numFollower, 1)) }.\n",
    "  reduceByKey { case ((accFollowers1, c1), (accFollowers2, c2)) => (accFollowers1 + accFollowers2, c1 + c2) }.\n",
    "  mapValues { case (accF, c) => accF / c }\n",
    "\n",
    "spark.time {\n",
    "  jobAggregationBeforeJoinAndPartitioner.collect.foreach {\n",
    "    case (0, avgFollowers) => println(s\"Playlist influenced most by Track's popularity has $avgFollowers followers on average\")\n",
    "    case (1, avgFollowers) => println(s\"Playlist influenced most by popularity of the year has $avgFollowers followers on average\")\n",
    "    case (2, avgFollowers) => println(s\"Playlist influenced most by Artist's popularity has $avgFollowers followers on average\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all the optimizations\n",
    "\n",
    "The final job has been executed merging the optimizations described above.\n",
    "\n",
    "- **RDD caching**: `Album` and `Track`\n",
    "- **Aggregate before join**\n",
    "- **Hash partitioning of biggest RDD**\n",
    "\n",
    "The job has been executed in `~7.3 min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpersistRDD()\n",
    "\n",
    "val finalTrackRddCached = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity, t.artists))).cache()\n",
    "val finalAlbumRddCached = albumRdd.map(a => (a.uri, a.release_year)).cache()\n",
    "\n",
    "val avgPopPerYear = finalTrackRddCached.\n",
    "  join(finalAlbumRddCached).\n",
    "  map { case (albumUri, ((trackUri, popularity, _), releaseYear)) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count } // year -> avg popularity of tracks in that year\n",
    "\n",
    "val trackWithArtistPopularity = finalTrackRddCached.flatMap(t => t._2._3.split('|').map(artistUri => (artistUri, t._2._1))).\n",
    "  join(artistRdd.map(a => (a.uri, a.popularity))).\n",
    "  map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count } // track -> avg popularity of artists of that track\n",
    "\n",
    "import org.apache.spark.HashPartitioner\n",
    " \n",
    "val finalJob = finalTrackRddCached.map(t => (t._1, (t._2._1, t._2._2))).\n",
    "  join(finalAlbumRddCached).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(avgPopPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }.\n",
    "  join(trackWithArtistPopularity).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).partitionBy(new HashPartitioner(6))).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear, avgArtistPopularity), pid)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity))}.\n",
    "  aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "    { case ((accPop, accAvgPopInYear, accAvgArtistPop, count), (popularity, avgPopularityInYear, avgArtistPopularity)) =>\n",
    "      (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, count + 1)\n",
    "    },\n",
    "    { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, count2)) =>\n",
    "      (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, count1 + count2)\n",
    "    }\n",
    "  ).\n",
    "  mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count) }.\n",
    "  map {\n",
    "    case (pid, (avgPop, avgPopInYear, avgArtistPop)) =>\n",
    "      val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "      val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "      (pid, indexOfBestAvg)\n",
    "  }.\n",
    "  join(playlistRdd.map(p => (p.pid, p.num_follower))).\n",
    "  map { case (pid, (indexOfBestAvg, numFollower)) => (indexOfBestAvg, (numFollower, 1)) }.\n",
    "  reduceByKey { case ((accFollowers1, c1), (accFollowers2, c2)) => (accFollowers1 + accFollowers2, c1 + c2) }.\n",
    "  mapValues { case (accF, c) => accF / c }\n",
    "\n",
    "spark.time {\n",
    "  finalJob.collect.foreach {\n",
    "    case (0, avgFollowers) => println(s\"Playlist influenced most by Track's popularity has $avgFollowers followers on average\")\n",
    "    case (1, avgFollowers) => println(s\"Playlist influenced most by popularity of the year has $avgFollowers followers on average\")\n",
    "    case (2, avgFollowers) => println(s\"Playlist influenced most by Artist's popularity has $avgFollowers followers on average\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Job 1 result](./img/job1_results.png)\n",
    "\n",
    "Other tests have been done, such as:\n",
    "\n",
    "- repartitioning and coalescing of `Tracks`\n",
    "- repartitioning and coalescing after aggregation during the job part\n",
    "- unpersisting the RDDs when they are not needed anymore\n",
    "\n",
    "But the best performance has been reached with the optimizations described above.\n",
    "\n",
    "Here the resume of the execution time:\n",
    "\n",
    "| Job                                           | Execution Time |\n",
    "|-----------------------------------------------|----------------|\n",
    "| Original Job                                  | 9.7 min        |\n",
    "| Aggregate before join                         | 13 min         |\n",
    "| Hash partitioning (6)                         | 11 min         |\n",
    "| Aggregate before join + Hash partitioning (6) | 7.9 min        |\n",
    "| Final Job                                     | 7.3 min        |\n",
    "| Final Job with repartition (300) on Tracks    | 7.7 min        |\n",
    "\n",
    "The achived speedup is $ S = \\frac{9.7 min}{7.3 min} = 1.32 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 2\n",
    "\n",
    "Given the following classes: slowly danceable (tempo <= 130BPM, danceability > 0.5), swiftly danceable (tempo >130BPM, danceability > 0.5), slowly undanceable (tempo <= 130BPM, danceability <= 0.5), swiftly undanceable (tempo >130BPM, danceability <= 0.5); and the various keys (C, C#/Db, ...).\n",
    "  for each class and (key ---OR--- range of followers) get:\n",
    "  - The number of playlist.\n",
    "  - Average playlist's explicitness percentage.\n",
    "  - Average number of tracks in playlist.\n",
    "  - Average number of playlist followers.\n",
    "  <!-- - Average playlist danceability.\n",
    "  - Average playlist tempo. -->\n",
    "  (The key of a playlist is the most present key among its tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toClass(tempo: Double, danceablility: Double): String = (tempo, danceablility) match {\n",
    "  case (t, d) if t <= 130 && d > 0.5 => \"slowly danceable\"\n",
    "  case (t, d) if t > 130 && d > 0.5 => \"swiftly danceable\"\n",
    "  case (t, d) if t <= 130 && d <= 0.5 => \"slowly undanceable\"\n",
    "  case (t, d) if t > 130 && d <= 0.5 => \"swiftly undanceable\"\n",
    "}\n",
    "\n",
    "def toKey(key: Int): String = Seq(\"C\", \"C#/Db\", \"D\", \"D#/Eb\", \"E\", \"F\", \"F#/Gb\", \"G\", \"G#/Ab\", \"A\", \"A#/Bb\", \"B\")(key)\n",
    "\n",
    "def incrementKey(map: Map[Int, Int], key: Int) = {\n",
    "    val currentValue = map.getOrElse(key, 0)\n",
    "    map.updated(key, currentValue + 1)\n",
    "}\n",
    "\n",
    "def joinMap(map1: Map[Int, Int], map2: Map[Int, Int]): Map[Int, Int] = map1.map { case(k, v) => (k, map2.getOrElse(k, 0) + v) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val features = featureRdd.map(t => (t.uri, (t.tempo, t.danceability, t.key))).\n",
    "  join(trackRdd.map(t => (t.uri, t.explicit))).\n",
    "  map { case (uri, ((t, d, k), e)) => (uri, (t, d, k, e)) }\n",
    "\n",
    "val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid))\n",
    "\n",
    "val playlistWithFollowers = playlistRdd.map(p => (p.pid, p.num_follower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First implementation\n",
    "\n",
    "Following the self-join pattern we computed the `playlistClasses` rdd by joining the `tracksInPlaylist`'s rdd with the `features`'s one and then aggregating on the playlist ID (pid) in order to compute the average tempo and danceability of each playlist. \n",
    "After that, we joined `tracksInPlaylistWithFeatures` with `playlistWithFollowers` so that by aggregating on the **pid** we could compute the ratio of explict songs and the number of followers of each playlist. Finally, we joined this last result with `playlistClasses` so that we could calculate per each of those classes the number of playlist belonging to the class, the average of explicit song ratio and number of followers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute the class of each playlist\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features)\n",
    "\n",
    "val playlistClasses = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (t, d, k, _))) => (pid, (t, d, k)) }.\n",
    "    aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0))(\n",
    "        { case ((accT, accD, accK, c), (t, d, k)) => (accT + t, accD + d, incrementKey(accK, k), c + 1) },\n",
    "        { case ( (accT1, accD1, accK1, c1), (accT2, accD2, accK2, c2)) => (accT1 + accT2, accD1+ accD2, joinMap(accK1, accK2), c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accT, accD, accK, c) => (toKey(accK.maxBy(_._2)._1), toClass(accT/c, accD/c)) } //(pid, (k, cls))\n",
    "\n",
    "val job1 = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (_,_,_, e))) => (pid, if (e) 1 else 0) }.\n",
    "    join(playlistWithFollowers). // (pid, (e, num_followers)) --> consider --J&A-- or A&J\n",
    "    aggregateByKey((0, 0, 0))(\n",
    "        { case ((accE, nF, c), (e, f)) => (accE + e, f, c + 1)  },\n",
    "        { case ((accE1, nF, c1), (accE2, _, c2)) => (accE1 + accE2, nF, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, nF, c) => (accE/c, nF) }. // (pid, avgE, NF)\n",
    "    join(playlistClasses). // (pid ((avgE, NF), (k, cls)))\n",
    "    map { case (pid, ((avgE, nF), (k, cls))) => ((k, cls), (avgE, nF))}.\n",
    "    aggregateByKey((0.0, 0, 0))(\n",
    "        { case ((accE, accF, c), (e, f)) => (accE + e, accF + f, c+1) },\n",
    "        { case ((accE1, accF1, c1), (accE2, accF2, c2)) => (accE1 + accE2, accF1 + accF2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, accF, c) => (accE/c, accF/c, c) }\n",
    "    \n",
    "val result = job1.collect    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job completed in around `22 minutes`. This is surely due the size of the input data, in particular those regarding the tracks in playlist which is `8.6GB` large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step toward optimizing execution time we noticed that we accessed the result of the join between `tracksInPlaylist` and `features` multiple times. So we dediced to cache it in order to avoid loading the dataset more than once and thus avoiding waste of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute the class of each playlist\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).cache\n",
    "\n",
    "val playlistClasses = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (t, d, k, _))) => (pid, (t, d, k)) }.\n",
    "    aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0))(\n",
    "        { case ((accT, accD, accK, c), (t, d, k)) => (accT + t, accD + d, incrementKey(accK, k), c + 1) },\n",
    "        { case ( (accT1, accD1, accK1, c1), (accT2, accD2, accK2, c2)) => (accT1 + accT2, accD1 + accD2, joinMap(accK1, accK2), c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accT, accD, accK, c) => (toKey(accK.maxBy(_._2)._1), toClass(accT/c, accD/c)) } //(pid, (k, cls))\n",
    "\n",
    "val job2 = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (_,_,_, e))) => (pid, if (e) 1 else 0) }.\n",
    "    join(playlistWithFollowers). // (pid, (e, num_followers))\n",
    "    aggregateByKey((0, 0, 0))(\n",
    "        { case ((accE, nF, c), (e, f)) => (accE + e, f, c + 1)  },\n",
    "        { case ((accE1, nF, c1), (accE2, _, c2)) => (accE1 + accE2, nF, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, nF, c) => (accE/c, nF) }. // (pid, avgE, NF)\n",
    "    join(playlistClasses). // (pid ((avgE, NF), (k, cls)))\n",
    "    map { case (pid, ((avgE, nF), (k, cls))) => ((k, cls), (avgE, nF))}.\n",
    "    aggregateByKey((0.0, 0, 0))(\n",
    "        { case ((accE, accF, c), (e, f)) => (accE + e, accF + f, c+1) },\n",
    "        { case ((accE1, accF1, c1), (accE2, accF2, c2)) => (accE1 + accE2, accF1 + accF2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, accF, c) => (accE/c, accF/c, c) }\n",
    "    \n",
    "val result = job2.collect    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No reduction in time was registered as the job took still `22 minutes` to perform. We therefore tried to change the execution plan. In the previous job, when joining `tracksInPlaylistWithFeatures` with `playlistFollowers`, we performed the so called `Join & Aggregate`. So we tried to perform the `Aggregate and Join` to see if there was any benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute the class of each playlist\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).cache\n",
    "\n",
    "val playlistClasses = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (t, d, k, _))) => (pid, (t, d, k)) }.\n",
    "    aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0))(\n",
    "        { case ((accT, accD, accK, c), (t, d, k)) => (accT + t, accD + d, incrementKey(accK, k), c + 1) },\n",
    "        { case ( (accT1, accD1, accK1, c1), (accT2, accD2, accK2, c2)) => (accT1 + accT2, accD1 + accD2, joinMap(accK1, accK2), c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accT, accD, accK, c) => (toKey(accK.maxBy(_._2)._1), toClass(accT/c, accD/c)) } //(pid, (k, cls))\n",
    "\n",
    "val playlistsWithClasses = playlistWithFollowers.join(playlistClasses).mapValues { case (f, (k, cls)) => (k, cls, f) }\n",
    "\n",
    "val job3 = tracksInPlaylistWithFeatures.\n",
    "    map { case (t_uri, (pid, (_,_,_, e))) => (pid, if (e) 1 else 0) }.\n",
    "    aggregateByKey((0.0, 0))(\n",
    "        { case ((accE, c), e) => (accE + e, c + 1)},\n",
    "        { case ((accE1, c1), (accE2, c2)) => (accE1 + accE2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, c) => accE / c }.\n",
    "    join(playlistsWithClasses). // (pid, (avgE, (k, cls, f)))\n",
    "    map { case (pid, (avgE, (k, cls, nF))) => ((k, cls), (avgE, nF))}.\n",
    "    aggregateByKey((0.0, 0, 0))(\n",
    "        { case ((accE, accF, c), (e, f)) => (accE + e, accF + f, c+1) },\n",
    "        { case ((accE1, accF1, c1), (accE2, accF2, c2)) => (accE1 + accE2, accF1 + accF2, c1 + c2) }\n",
    "    ).\n",
    "    mapValues { case (accE, accF, c) => (accE/c, accF/c, c) }\n",
    "    \n",
    "val result = job3.collect    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing an *Aggregate and Join* the execution time increase of `2 minutes`. So ultimately we tried to compute the playlist classes and the requested averages in a single aggregation step before joining with `playlistWithFollowers` and aggregating to obtain the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).\n",
    "    map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithClasses = tracksInPlaylistWithFeatures.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (toKey(k.maxBy(_._2)._1), toClass(accT/c, accD/c), ec/c, c) }) // (pid, (k, class, avgE, c))\n",
    "\n",
    "val job4 = playlistWithFollowers.join(tracksInPlaylistWithClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) } // ((k, class), (avgF, avgE, avgTC, c))\n",
    "        \n",
    "val result = job4.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last job took in total `20 minutes`. Overall we obtain a `4 minutes` improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further optimizations\n",
    "\n",
    "Since the last job was in term of time consuption more efficient, it has been choosen for further optimizations.\n",
    "First of all, by looking at the job details on SparkUI it becomes evident that the stage that is more costsly is the join between the `tracksInPlaylist` with the `features`. This may be due to the fact that the `tracksInPlaylist` form a total of 277 block thus 277 tasks. This very likely results in a big scheduling overhead and to the generation of a lot of intermediate files due to the shuffling strategy. So we tried reducing the number of partitions. \n",
    "\n",
    "### Coalescing the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpersistRDDs()\n",
    "// val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(150)\n",
    "// val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(50)\n",
    "// val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(10)\n",
    "val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).coalesce(6)\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).\n",
    "    map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithClasses = tracksInPlaylistWithFeatures.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (toKey(k.maxBy(_._2)._1), toClass(accT/c, accD/c), ec/c, c) }) // (pid, (k, class, avgE, c))\n",
    "\n",
    "val job5 = playlistWithFollowers.join(tracksInPlaylistWithClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) } // ((k, class), (avgF, avgE, avgTC, c))\n",
    "job5.collect        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the the result of the various executions:\n",
    "\n",
    "|N. of tasks |Join step shuffle data size|Exection time|\n",
    "|------------|---------------------------|-------------|\n",
    "|150|12.2 GB|17 min|\n",
    "|50|7.0 GB|13 min|\n",
    "|10|1704.8 MB|14 min|\n",
    "|6|---|8.6 min|\n",
    "|5|1013.7MB|9.3 min|\n",
    "\n",
    "So the best number of partitions seems to be `6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing a partition criteria\n",
    "\n",
    "Next we tried to enforce the same partion criteria on the rdd involved in the join. We tried with different number of partiotions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "//val p = new HashPartitioner(50)\n",
    "//val p = new HashPartitioner(10)\n",
    "//val p = new HashPartitioner(6)\n",
    "val p = new HashPartitioner(5)\n",
    "\n",
    "unpersistRDDs()\n",
    "\n",
    "val features = featureRdd.map(t => (t.uri, (t.tempo, t.danceability, t.key))).\n",
    "  join(trackRdd.map(t => (t.uri, t.explicit))).\n",
    "  map { case (uri, ((t, d, k), e)) => (uri, (t, d, k, e)) }.partitionBy(p)\n",
    "val tracksInPlaylist = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).partitionBy(p)\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithFeatures = tracksInPlaylist.join(features).\n",
    "    map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }\n",
    "\n",
    "\n",
    "val tracksInPlaylistWithClasses = tracksInPlaylistWithFeatures.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (toKey(k.maxBy(_._2)._1), toClass(accT/c, accD/c), ec/c, c) }).partitionBy(p) // (pid, (k, class, avgE, c))\n",
    "\n",
    "val job4 = playlistWithFollowers.partitionBy(p).join(tracksInPlaylistWithClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) } // ((k, class), (avgF, avgE, avgTC, c))\n",
    "val result = job4.collect        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the the result of the various executions:\n",
    "\n",
    "|N. of tasks |Join step shuffle data size|Exection time|\n",
    "|------------|---------------------------|-------------|\n",
    "|50|7.0 GB|13 min|\n",
    "|10|1704.8 MB|12 min|\n",
    "|6|---|7.5 min|\n",
    "|5|1013.7 MB|7.5 min|\n",
    "\n",
    "So the best number of partitions seems to be `6 or 5`.\n",
    "\n",
    "So overall we obtain a speed-up of $ S=\\frac{20 min}{7.5 min} = 2.66 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
