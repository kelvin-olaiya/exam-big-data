{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Big Data Exam Report @ UniBo a.y. 2023/2024\n",
    "\n",
    "- Manuel Andruccioli\n",
    "- Kelvin Olaiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "val sc = new SparkContext(\"local[*]\", \"BigDataExam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures and definitions\n",
    "\n",
    "### Utility function for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T17:09:52.217563Z",
     "start_time": "2024-04-20T17:09:50.451214Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCharIndexes(line: String, char: Char): Seq[Int] = line.zipWithIndex.filter(_._1 == char).map(_._2) \n",
    "def splitAt(s: String, indices: Seq[Int]): Seq[String] = indices match {\n",
    "  case h +: t => s.splitAt(h) match {\n",
    "    case (a, b) => a +: splitAt(b, t.map(_ - h))\n",
    "  }\n",
    "  case Nil => Seq(s)\n",
    "}\n",
    "def parseCSVLine(l: String): Seq[String] = {\n",
    "  val apices = getCharIndexes(l, '\"').grouped(2).map { case Seq(a, b) => (a, b) }.toSeq\n",
    "  val commas = getCharIndexes(l, ',').filter(i => !apices.exists { case (a, b) => a < i && i < b })\n",
    "  return splitAt(l, commas).map(_.dropWhile(s => s == ',' || s == ' ')).map(_.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:25:21.122005Z",
     "start_time": "2024-04-21T10:25:19.798787Z"
    }
   },
   "outputs": [],
   "source": [
    "case class Track(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  duration: Int,\n",
    "  explicit: Boolean,\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  album_uri: String,\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Track {\n",
    "  def fromCSVLine(line: String): Option[Track] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, duration, explicit, artists, available_markets, album_uri, popularity) =>\n",
    "        try {\n",
    "          Some(Track(uri, name, duration.toInt, explicit.toBoolean, artists, available_markets, album_uri, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Playlist(\n",
    "  pid: Int,\n",
    "  name: String,\n",
    "  num_follower: Int,\n",
    ")\n",
    "\n",
    "object Playlist {\n",
    "  def fromCSVLine(line: String): Option[Playlist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, name, num_follower) =>\n",
    "        try {\n",
    "            Some(Playlist(pid.toInt, name, num_follower.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class TrackInPlaylist(\n",
    "  pid: Int,\n",
    "  track_uri: String,\n",
    "  pos: Int,\n",
    ")\n",
    "\n",
    "object TrackInPlaylist {\n",
    "  def fromCSVLine(line: String): Option[TrackInPlaylist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, track_uri, pos) =>\n",
    "        try {\n",
    "          Some(TrackInPlaylist(pid.toInt, track_uri, pos.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Artist(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  followers: Int,\n",
    "  genres: String,             // List of genres, separated by |\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Artist {\n",
    "  def fromCSVLine(line: String): Option[Artist] =\n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, followers, genres, popularity) =>\n",
    "        try {\n",
    "          Some(Artist(uri, name, followers.toInt, genres, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Album(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  album_type: String,         // album, compilation, single.\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  release_year: String,\n",
    "  total_tracks: Int,\n",
    ")\n",
    "\n",
    "object Album {\n",
    "  def fromCSVLine(line: String): Option[Album] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, album_type, artists, available_markets, release_year, total_tracks) =>\n",
    "        try {\n",
    "          Some(Album(uri, name, album_type, artists, available_markets, release_year, total_tracks.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Feature(\n",
    "  uri: String,\n",
    "  key: Int,\n",
    "  loudness: Double,\n",
    "  tempo: Double,\n",
    "  mode: Boolean,\n",
    "  danceability: Double,\n",
    "  valence: Double,\n",
    "  instrumentalness: Double,\n",
    "  liveness: Double,\n",
    "  acousticness: Double,\n",
    "  energy: Double,\n",
    "  speechiness: Double,\n",
    ")\n",
    "\n",
    "object Feature {\n",
    "  def fromCSVLine(line: String): Option[Feature] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, key, loudness, tempo, mode, danceability, valence, instrumentalness, liveness, acousticness, energy, speechiness) =>\n",
    "        try {\n",
    "          Some(Feature(uri, key.toInt, loudness.toDouble, tempo.toDouble, mode.toInt == 1, danceability.toDouble, valence.toDouble, instrumentalness.toDouble, liveness.toDouble, acousticness.toDouble, energy.toDouble, speechiness.toDouble))\n",
    "        } catch {\n",
    "          case e: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T17:24:25.863233Z",
     "start_time": "2024-04-20T17:24:17.899357Z"
    }
   },
   "outputs": [],
   "source": [
    "val datasetPath = \"dataset/\"\n",
    "\n",
    "val albumRdd = sc.textFile(s\"${datasetPath}albums.csv\").flatMap(Album.fromCSVLine)\n",
    "val artistRdd = sc.textFile(s\"${datasetPath}artists.csv\").flatMap(Artist.fromCSVLine)\n",
    "val featureRdd = sc.textFile(s\"${datasetPath}features.csv\").flatMap(Feature.fromCSVLine)\n",
    "val playlistRdd = sc.textFile(s\"${datasetPath}playlists.csv\").flatMap(Playlist.fromCSVLine)\n",
    "val trackInPlaylistRdd = sc.textFile(s\"${datasetPath}tracks_in_playlists.csv\").flatMap(TrackInPlaylist.fromCSVLine)\n",
    "val trackRdd = sc.textFile(s\"${datasetPath}tracks.csv\").flatMap(Track.fromCSVLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*val albumRddCached = albumRdd.cache()\n",
    "val artistRddCached = artistRdd.cache()\n",
    "val featureRddCached = featureRdd.cache()\n",
    "val playlistRddCached = playlistRdd.cache()\n",
    "val trackInPlaylistRddCached = trackInPlaylistRdd.cache()\n",
    "val trackRddCached = trackRdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Number of Albums: ${albumRddCached.count()}\")\n",
    "println(s\"Number of Artists: ${artistRddCached.count()}\")\n",
    "println(s\"Number of Track's Feature: ${featureRddCached.count()}\")\n",
    "println(s\"Number of Playlist: ${playlistRddCached.count()}\")\n",
    "println(s\"Number of Tracks add in Playlists: ${trackInPlaylistRddCached.count()}\")\n",
    "println(s\"Number of Tracks: ${trackRddCached.count()}\")*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "- Date le seguenti metriche:\n",
    "  - popolarità della traccia\n",
    "  - popolarità media delle tracce in anno\n",
    "  - popolarità dell'artista (se più artisti, media di essi)\n",
    "Capire come una playlist viene influenzata maggiormente dalle precedenti metriche, mediando i valori delle tracce di cui è composta. Inoltre, aggregare le playlist sull'influenza precedentemente calcolata, mediando per il numero di followers delle playlist.\n",
    "La query permette di rispondere alla seguente domanda:\n",
    "una playlist influenzata maggiormente dalla popolarità delle tracce ha in media 500 followers. (stessa cosa per le altre due metriche di partenza)\n",
    " \n",
    "- Given the following classes: slowly danceable (tempo <= 130BPM, danceability > 0.5), swiftly danceable (tempo >130BPM, danceability > 0.5), slowly undanceable (tempo <= 130BPM, danceability <= 0.5), swiftly undanceable (tempo >130BPM, danceability <= 0.5); and the various keys (C, C#/Db, ...).\n",
    "  for each class and (key ---OR--- range of followers) get:\n",
    "    - The number of playlist.\n",
    "    - Average playlist's percentage.\n",
    "    - Percentage of explicit songs.\n",
    "    - Average number of playlist followers.\n",
    "    - Average tracks tempo\n",
    "    - Average tracks danceability\n",
    "  (The key of a playlist is the most present key among its tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trackWithAlbumUriAsKey = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity)))\n",
    "\n",
    "val avgPopPerYear = albumRdd.map(a => (a.uri, a.release_year)).\n",
    "        join(trackWithAlbumUriAsKey).\n",
    "        map { case (albumUri, (releaseYear, (trackUri, popularity))) => (releaseYear, popularity) }.\n",
    "        aggregateByKey((0, 0))(\n",
    "          { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "          { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "        ).mapValues { case (acc, count) => acc.toDouble / count } // year -> avg popularity of tracks in that year\n",
    "\n",
    "val trackWithPopularityAndAvgPopInYear = albumRdd.map(a => (a.uri, a.release_year)).\n",
    "        join(trackWithAlbumUriAsKey).\n",
    "        map { case (albumUri, (releaseYear, (trackUri, popularity))) => (releaseYear, (trackUri, popularity)) }.\n",
    "        join(avgPopPerYear).\n",
    "        map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val artistWithPopularity = artistRdd.map(a => (a.uri, a.popularity))\n",
    "\n",
    "val trackWithArtistPopularity = trackRdd.flatMap(t => t.artists.split('|').map(artistUri => (artistUri, t.uri))).\n",
    "        join(artistWithPopularity).\n",
    "        map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "        aggregateByKey((0, 0))(\n",
    "          { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "          { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "        ).mapValues { case (acc, count) => acc.toDouble / count } // track -> avg popularity of artists of that track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trackWithPid = trackInPlaylistRdd.map(t => (t.track_uri, t.pid))\n",
    "\n",
    "val popAvgArtistPop = trackWithPopularityAndAvgPopInYear.join(trackWithArtistPopularity).\n",
    "        map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }\n",
    "\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "trackWithPid.partitionBy(new HashPartitioner(1)).\n",
    "        join(popAvgArtistPop).\n",
    "        map {\n",
    "            case (trackUri, (pid, (popularity, avgPopularityInYear, avgArtistPopularity))) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity))\n",
    "        }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accPop, accAvgPopInYear, accAvgArtistPop, count), (popularity, avgPopularityInYear, avgArtistPopularity)) =>  \n",
    "              (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, count + 1)\n",
    "          },\n",
    "          { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, count2)) =>\n",
    "            (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, count1 + count2)\n",
    "          }\n",
    "        ).\n",
    "        mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count) }.\n",
    "        map {\n",
    "            case (pid, (avgPop, avgPopInYear, avgArtistPop)) =>\n",
    "              val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "              val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "              (pid, indexOfBestAvg)\n",
    "        }.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 2\n",
    "\n",
    "Given the following classes: slowly danceable (tempo <= 130BPM, danceability > 0.5), swiftly danceable (tempo >130BPM, danceability > 0.5), slowly undanceable (tempo <= 130BPM, danceability <= 0.5), swiftly undanceable (tempo >130BPM, danceability <= 0.5); and the various keys (C, C#/Db, ...).\n",
    "  for each class and (key ---OR--- range of followers) get:\n",
    "  - The number of playlist.\n",
    "  - Average playlist's explicitness percentage.\n",
    "  - Average number of tracks in playlist.\n",
    "  - Average number of playlist followers.\n",
    "  <!-- - Average playlist danceability.\n",
    "  - Average playlist tempo. -->\n",
    "  (The key of a playlist is the most present key among its tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:30:32.044772Z",
     "start_time": "2024-04-20T17:47:03.188452Z"
    }
   },
   "outputs": [],
   "source": [
    "def toClass(tempo: Double, danceablility: Double): String = (tempo, danceablility) match {\n",
    "  case (t, d) if t <= 130 && d > 0.5 => \"slowly danceable\"\n",
    "  case (t, d) if t > 130 && d > 0.5 => \"swiftly danceable\"\n",
    "  case (t, d) if t <= 130 && d <= 0.5 => \"slowly undanceable\"\n",
    "  case (t, d) if t > 130 && d <= 0.5 => \"swiftly undanceable\"\n",
    "}\n",
    "\n",
    "def joinMap(map1: Map[Int, Int], map2: Map[Int, Int]): Map[Int, Int] = map1.map { case(k, v) => (k, map2.getOrElse(k, 0) + v) }\n",
    "\n",
    "val features = featureRdd.map(t => (t.uri, (t.tempo, t.danceability, t.key))).\n",
    "  join(trackRdd.map(t => (t.uri, t.explicit))).\n",
    "  map { case (uri, ((t, d, k), e)) => (uri, (t, d, k, e)) }\n",
    "\n",
    "val trackInPlaylistWithFeatures = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).join(features)\n",
    "\n",
    "val playlistClasses = trackInPlaylistWithFeatures.\n",
    "        map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, ks.updatedWith(k)(_.map(_+1)), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (accT/c, accD/c, k.maxBy(_._2)._1, ec/c, c) }).\n",
    "        map { case (pid, (avgT, avgD, k, avgE, c)) => (pid, (k, toClass(avgT, avgD), avgE, c)) }// (pid, (k, class, avgE, c))\n",
    "\n",
    "playlistRdd.map(p => (p.pid, p.num_follower)).join(playlistClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) }. // ((k, class), (avgF, avgE, avgTC, c))\n",
    "        collect.foreach(println)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
