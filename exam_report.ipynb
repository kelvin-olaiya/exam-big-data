{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Big Data Exam Report @ UniBo a.y. 2023/2024\n",
    "\n",
    "- Manuel Andruccioli\n",
    "- Kelvin Olaiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "// DO NOT RUN THIS CELL -- ONLY FOR TYPE CHECKER\n",
    "import org.apache.spark.SparkContext\n",
    "val sc = new SparkContext(\"local[*]\", \"BigDataExam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.applicationId\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures and definitions\n",
    "\n",
    "### Utility function for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharIndexes(line: String, char: Char): Seq[Int] = line.zipWithIndex.filter(_._1 == char).map(_._2) \n",
    "def splitAt(s: String, indices: Seq[Int]): Seq[String] = indices match {\n",
    "  case h +: t => s.splitAt(h) match {\n",
    "    case (a, b) => a +: splitAt(b, t.map(_ - h))\n",
    "  }\n",
    "  case Nil => Seq(s)\n",
    "}\n",
    "def parseCSVLine(l: String): Seq[String] = {\n",
    "  val apices = getCharIndexes(l, '\"').grouped(2).map { case Seq(a, b) => (a, b) }.toSeq\n",
    "  val commas = getCharIndexes(l, ',').filter(i => !apices.exists { case (a, b) => a < i && i < b })\n",
    "  return splitAt(l, commas).map(_.dropWhile(s => s == ',' || s == ' ')).map(_.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Track(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  duration: Int,\n",
    "  explicit: Boolean,\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  album_uri: String,\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Tracks {\n",
    "  def fromCSVLine(line: String): Option[Track] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, duration, explicit, artists, available_markets, album_uri, popularity) =>\n",
    "        try {\n",
    "          Some(Track(uri, name, duration.toInt, explicit.toBoolean, artists, available_markets, album_uri, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Playlist(\n",
    "  pid: Int,\n",
    "  name: String,\n",
    "  num_follower: Int,\n",
    ")\n",
    "\n",
    "object Playlists {\n",
    "  def fromCSVLine(line: String): Option[Playlist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, name, num_follower) =>\n",
    "        try {\n",
    "            Some(Playlist(pid.toInt, name, num_follower.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class TrackInPlaylist(\n",
    "  pid: Int,\n",
    "  track_uri: String,\n",
    "  pos: Int,\n",
    ")\n",
    "\n",
    "object TrackInPlaylists {\n",
    "  def fromCSVLine(line: String): Option[TrackInPlaylist] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(pid, track_uri, pos) =>\n",
    "        try {\n",
    "          Some(TrackInPlaylist(pid.toInt, track_uri, pos.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Artist(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  followers: Int,\n",
    "  genres: String,             // List of genres, separated by |\n",
    "  popularity: Int,\n",
    ")\n",
    "\n",
    "object Artists {\n",
    "  def fromCSVLine(line: String): Option[Artist] =\n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, followers, genres, popularity) =>\n",
    "        try {\n",
    "          Some(Artist(uri, name, followers.toInt, genres, popularity.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Album(\n",
    "  uri: String,\n",
    "  name: String,\n",
    "  album_type: String,         // album, compilation, single.\n",
    "  artists: String,            // List of artists uri, separated by |\n",
    "  available_markets: String,  // List of markets, separated by |\n",
    "  release_year: String,\n",
    "  total_tracks: Int,\n",
    ")\n",
    "\n",
    "object Albums {\n",
    "  def fromCSVLine(line: String): Option[Album] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, name, album_type, artists, available_markets, release_year, total_tracks) =>\n",
    "        try {\n",
    "          Some(Album(uri, name, album_type, artists, available_markets, release_year, total_tracks.toInt))\n",
    "        } catch {\n",
    "          case _: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "case class Feature(\n",
    "  uri: String,\n",
    "  key: Int,\n",
    "  loudness: Double,\n",
    "  tempo: Double,\n",
    "  mode: Boolean,\n",
    "  danceability: Double,\n",
    "  valence: Double,\n",
    "  instrumentalness: Double,\n",
    "  liveness: Double,\n",
    "  acousticness: Double,\n",
    "  energy: Double,\n",
    "  speechiness: Double,\n",
    ")\n",
    "\n",
    "object Features {\n",
    "  def fromCSVLine(line: String): Option[Feature] = \n",
    "    parseCSVLine(line) match {\n",
    "      case Seq(uri, key, loudness, tempo, mode, danceability, valence, instrumentalness, liveness, acousticness, energy, speechiness) =>\n",
    "        try {\n",
    "          Some(Feature(uri, key.toInt, loudness.toDouble, tempo.toDouble, mode.toInt == 1, danceability.toDouble, valence.toDouble, instrumentalness.toDouble, liveness.toDouble, acousticness.toDouble, energy.toDouble, speechiness.toDouble))\n",
    "        } catch {\n",
    "          case e: Throwable => None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val datasetPath = \"dataset/\"\n",
    "val outputPath = s\"${datasetPath}output/\"\n",
    "\n",
    "val albumRdd = sc.textFile(s\"${datasetPath}albums.csv\").flatMap(Albums.fromCSVLine)\n",
    "val artistRdd = sc.textFile(s\"${datasetPath}artists.csv\").flatMap(Artists.fromCSVLine)\n",
    "val featureRdd = sc.textFile(s\"${datasetPath}features.csv\").flatMap(Features.fromCSVLine)\n",
    "val playlistRdd = sc.textFile(s\"${datasetPath}playlists.csv\").flatMap(Playlists.fromCSVLine)\n",
    "val trackInPlaylistRdd = sc.textFile(s\"${datasetPath}tracks_in_playlists.csv\").flatMap(TrackInPlaylists.fromCSVLine)\n",
    "val trackRdd = sc.textFile(s\"${datasetPath}tracks.csv\").flatMap(Tracks.fromCSVLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val albumRddCached = albumRdd.cache()\n",
    "val artistRddCached = artistRdd.cache()\n",
    "val featureRddCached = featureRdd.cache()\n",
    "val playlistRddCached = playlistRdd.cache()\n",
    "val trackInPlaylistRddCached = trackInPlaylistRdd.cache()\n",
    "val trackRddCached = trackRdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Number of Albums: ${albumRddCached.count()}\")\n",
    "println(s\"Number of Artists: ${artistRddCached.count()}\")\n",
    "println(s\"Number of Track's Feature: ${featureRddCached.count()}\")\n",
    "println(s\"Number of Playlist: ${playlistRddCached.count()}\")\n",
    "println(s\"Number of Tracks add in Playlists: ${trackInPlaylistRddCached.count()}\")\n",
    "println(s\"Number of Tracks: ${trackRddCached.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Average number of tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"In Playlist: ${(trackInPlaylistRddCached.count().toDouble / playlistRddCached.count().toDouble).round}\")\n",
    "println(s\"In Album: ${(albumRddCached.map(_.total_tracks).sum() / albumRddCached.count().toDouble).round}\")\n",
    "println(s\"Per Artist (double counting on artist): ${\n",
    "  trackRddCached.flatMap(t => t.artists.split('|').map(a => (a, 1))).\n",
    "    reduceByKey(_ + _).\n",
    "    map { case (_, count) => count }.\n",
    "    mean().round}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Explicit vs Non-Explicit Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "trackRddCached.map(t => (t.explicit, 1)).\n",
    "  reduceByKey(_ + _).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}explicit_vs_non_explicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Explicit vs Non-Explicit Tracks](./img/explicit_vs_non_explicit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tracks per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumRddCached.map(a => a.release_year).\n",
    "  distinct().collect().sorted\n",
    "\n",
    "albumRddCached.map(a => (a.uri, a.release_year)).\n",
    "  join(trackRddCached.map(t => (t.album_uri, (t.uri, t.popularity)))).\n",
    "  map { case (albumUri, (releaseYear, (trackUri, popularity))) => (releaseYear, 1) }.\n",
    "  reduceByKey(_ + _).\n",
    "  coalesce(1).\n",
    "  sortByKey().\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}tracks_per_year.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Track per Year](./img/tracks_per_year.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Top\n",
    "\n",
    "#### Most followed Playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlistRddCached.map(p => (p.pid, p.name, p.num_follower)).\n",
    "  sortBy(_._3, ascending = false).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}playlist_with_followers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Top 10 playlist per followers](./img/top10_playlist_per_followers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Most popular Playlist\n",
    "\n",
    "The playlist popularity is calculated as the mean of the popularity of the tracks in the playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackInPlaylistRddCached.\n",
    "  map(t => (t.track_uri, t.pid)).\n",
    "  join(trackRddCached.map(t => (t.uri, t.popularity)))\n",
    "  .map { case (_, (pid, popularity)) => (pid, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }.\n",
    "  join(playlistRddCached.map(p => (p.pid, p.name))).\n",
    "  map { case (pid, (popularity, name)) => (pid, name, popularity) }.\n",
    "  sortBy(_._3, ascending = false).\n",
    "  coalesce(1).\n",
    "  toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(s\"${outputPath}playlist_with_popularity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Top playlist per popularity](./img/top_playlist_per_popularity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getPersistentRDDs.foreach(_._2.unpersist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Job 1\n",
    "\n",
    "Given the following metrics:\n",
    "\n",
    "  - track's popularity\n",
    "  - average popularity of tracks in year\n",
    "  - artist popularity (average if more artists are present in the track)\n",
    "\n",
    "Understand which metrics influence mostly the playlists, averaging the values of the tracks appearing in each.\n",
    "Also, aggregate the playlists on the previously calculated influence by averaging for the number of playlist followers.\n",
    "\n",
    "> **most influent**: the metric that has the highest average value for a playlist. \n",
    "\n",
    "The query let us answering the following question:\n",
    "a playlist influenced most by the popularity of the tracks has, on average, 500 followers. (Same for the other two metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Non optimized\n",
    "\n",
    "First of all, we need to calculate the `popularity of the year`.\n",
    "\n",
    "$$ \\texttt{Popularity of the year} = \n",
    "\\frac{\n",
    "  \\sum{\\texttt{popularity of tracks in that year}}\n",
    "}{\n",
    "  \\texttt{Number of tracks in that year}\n",
    "}\n",
    "$$\n",
    "\n",
    "Given the fact that the `release year` of a track is on the album, we need to join the track with the album to get it.\n",
    "After this first step, we can calculate the average popularity of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trackWithAlbumUriAsKey = trackRdd.map(t => (t.album_uri, (t.uri, t.popularity)))\n",
    "\n",
    "// year -> popularity of the year\n",
    "val popPerYear = albumRdd.map(a => (a.uri, a.release_year)).\n",
    "  join(trackWithAlbumUriAsKey).\n",
    "  map { case (albumUri, (releaseYear, (trackUri, popularity))) => (releaseYear, popularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, we need to get the `artist's popularity of a track`.\n",
    "Given the fact that a track can have multiple artists, the `artist's popularity of a track` will be a mean of the `artist popularity` of the artists in the track.\n",
    "\n",
    "$$ \\texttt{Artist's popularity of a track} =\n",
    "\\frac{\n",
    "  \\sum{\\texttt{popularity of artists in the track}}\n",
    "}{\n",
    "  \\texttt{Number of artists in the track}\n",
    "}\n",
    "$$\n",
    "\n",
    "> **Note**: The `artists_uri` is a denormalized field in the track, so we need to split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val artistWithPopularity = artistRdd.map(a => (a.uri, a.popularity))\n",
    "\n",
    "// track -> artist's popularity\n",
    "val trackWithArtistPopularity = trackRdd.flatMap(t => t.artists.split('|').map(artistUri => (artistUri, t.uri))).\n",
    "  join(artistWithPopularity).\n",
    "  map { case (artistUri, (trackUri, artistPopularity)) => (trackUri, artistPopularity) }.\n",
    "  aggregateByKey((0, 0))(\n",
    "    { case ((acc, count), popularity) => (acc + popularity, count + 1) },\n",
    "    { case ((acc1, count1), (acc2, count2)) => (acc1 + acc2, count1 + count2) }\n",
    "  ).mapValues { case (acc, count) => acc.toDouble / count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "At this point we can join all the data to get the `popularity`, `average popularity of the year` and `artist's popularity` of a track.\n",
    "\n",
    "$$\n",
    "\\texttt{track_uri} \\rightarrow (\\texttt{popularity}, \\texttt{average popularity of the year}, \\texttt{artist's popularity})\n",
    "$$\n",
    "\n",
    "After that, we can join the `track` with the `playlist` and perform the aggregation, achieving the average of the metrics for each playlist:\n",
    "\n",
    "$$\n",
    "\\texttt{pid} \\rightarrow (\\texttt{popularity}, \\texttt{average popularity of the year}, \\texttt{artist's popularity})\n",
    "$$\n",
    "\n",
    "Now, we can calculate the `most influent` metric for each playlist taking the maximum value of the metrics.\n",
    "  \n",
    "Finally, we can aggregate the playlists by the `most influent` metric and calculate the average number of followers for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-21T18:38:25.599372Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "trackWithAlbumUriAsKey.\n",
    "  join(albumRdd.map(a => (a.uri, a.release_year))).\n",
    "  map { case (albumUri, ((trackUri, popularity), releaseYear)) => (releaseYear, (trackUri, popularity)) }.\n",
    "  join(popPerYear).\n",
    "  map { case (releaseYear, ((trackUri, popularity), avgPopularityInYear)) => (trackUri, (popularity, avgPopularityInYear)) }.\n",
    "  join(trackWithArtistPopularity).\n",
    "  map { case (trackUri, ((popularity, avgPopularityInYear), avgArtistPopularity)) => (trackUri, (popularity, avgPopularityInYear, avgArtistPopularity)) }.\n",
    "  join(trackInPlaylistRdd.map(t => (t.track_uri, t.pid))).\n",
    "  map {\n",
    "      case (trackUri, ((popularity, avgPopularityInYear, avgArtistPopularity), pid)) => (pid, (popularity, avgPopularityInYear, avgArtistPopularity))\n",
    "  }.\n",
    "  aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "    { case ((accPop, accAvgPopInYear, accAvgArtistPop, count), (popularity, avgPopularityInYear, avgArtistPopularity)) =>  \n",
    "        (accPop + popularity, accAvgPopInYear + avgPopularityInYear, accAvgArtistPop + avgArtistPopularity, count + 1)\n",
    "    },\n",
    "    { case ((accPop1, accAvgPopInYear1, accAvgArtistPop1, count1), (accPop2, accAvgPopInYear2, accAvgArtistPop2, count2)) =>\n",
    "      (accPop1 + accPop2, accAvgPopInYear1 + accAvgPopInYear2, accAvgArtistPop1 + accAvgArtistPop2, count1 + count2)\n",
    "    }\n",
    "  ).\n",
    "  mapValues { case (accPop, accAvgPopInYear, accAvgArtistPop, count) => (accPop / count, accAvgPopInYear / count, accAvgArtistPop / count) }.\n",
    "  map {\n",
    "      case (pid, (avgPop, avgPopInYear, avgArtistPop)) =>\n",
    "        val maxAvg = Math.max(avgPop, Math.max(avgPopInYear, avgArtistPop))\n",
    "        val indexOfBestAvg = Seq(avgPop, avgPopInYear, avgArtistPop).indexWhere(_ >= maxAvg)\n",
    "        (pid, indexOfBestAvg)\n",
    "  }.\n",
    "  join(playlistRdd.map(p => (p.pid, p.num_follower))).\n",
    "  map { case (pid, (indexOfBestAvg, numFollower)) => (indexOfBestAvg, (numFollower, 1)) }.\n",
    "  reduceByKey { case ((accFollowers1, c1), (accFollowers2, c2)) => (accFollowers1 + accFollowers2, c1 + c2) }.\n",
    "  mapValues { case (accF, c) => accF / c }.\n",
    "  collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 2\n",
    "\n",
    "Given the following classes: slowly danceable (tempo <= 130BPM, danceability > 0.5), swiftly danceable (tempo >130BPM, danceability > 0.5), slowly undanceable (tempo <= 130BPM, danceability <= 0.5), swiftly undanceable (tempo >130BPM, danceability <= 0.5); and the various keys (C, C#/Db, ...).\n",
    "  for each class and (key ---OR--- range of followers) get:\n",
    "  - The number of playlist.\n",
    "  - Average playlist's explicitness percentage.\n",
    "  - Average number of tracks in playlist.\n",
    "  - Average number of playlist followers.\n",
    "  <!-- - Average playlist danceability.\n",
    "  - Average playlist tempo. -->\n",
    "  (The key of a playlist is the most present key among its tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toClass(tempo: Double, danceablility: Double): String = (tempo, danceablility) match {\n",
    "  case (t, d) if t <= 130 && d > 0.5 => \"slowly danceable\"\n",
    "  case (t, d) if t > 130 && d > 0.5 => \"swiftly danceable\"\n",
    "  case (t, d) if t <= 130 && d <= 0.5 => \"slowly undanceable\"\n",
    "  case (t, d) if t > 130 && d <= 0.5 => \"swiftly undanceable\"\n",
    "}\n",
    "\n",
    "def joinMap(map1: Map[Int, Int], map2: Map[Int, Int]): Map[Int, Int] = map1.map { case(k, v) => (k, map2.getOrElse(k, 0) + v) }\n",
    "def incrementOnKey(map: Map[Int, Int], key: Int) = {\n",
    "  val current = map.getOrElse(key, 0)\n",
    "  map.updated(key, current + 1)\n",
    "}\n",
    "\n",
    "val features = featureRdd.map(t => (t.uri, (t.tempo, t.danceability, t.key))).\n",
    "  join(trackRdd.map(t => (t.uri, t.explicit))).\n",
    "  map { case (uri, ((t, d, k), e)) => (uri, (t, d, k, e)) }\n",
    "\n",
    "val trackInPlaylistWithFeatures = trackInPlaylistRdd.map(t => (t.track_uri, t.pid)).join(features)\n",
    "\n",
    "val playlistClasses = trackInPlaylistWithFeatures.\n",
    "        map { case (trackUri, (pid, (t, d, k, e))) => (pid, (t, d, k, e)) }.\n",
    "        aggregateByKey((0.0, 0.0, (0 to 11).map((_, 0)).toMap, 0.0, 0))(\n",
    "          { case ((accT, accD, ks, ec, c), (t, d, k, e)) => (accT+t, accD+d, incrementOnKey(ks, k), ec+(if (e) 1 else 0), c+1) },\n",
    "          { case ((accT1, accD1, k1, ec1, c1), (accT2, accD2, k2, ec2, c2)) => (accT1+accT2, accD1+accD2, joinMap(k1, k2), ec1+ec2, c1+c2) }).\n",
    "        mapValues({ case (accT, accD, k, ec, c) => (accT/c, accD/c, k.maxBy(_._2)._1, ec/c, c) }).\n",
    "        map { case (pid, (avgT, avgD, k, avgE, c)) => (pid, (k, toClass(avgT, avgD), avgE, c)) }// (pid, (k, class, avgE, c))\n",
    "\n",
    "playlistRdd.map(p => (p.pid, p.num_follower)).join(playlistClasses). // (pid, (num_follower, (k, class, avgE, c)))\n",
    "        map { case (pid, (num_follower, (k, cls, avgE, tc))) => ((k, cls), (num_follower, avgE, tc)) }.\n",
    "        aggregateByKey((0.0, 0.0, 0.0, 0))(\n",
    "          { case ((accF, accE, accTC, c), (f, e, tc)) => (accF+f, accE+e, accTC+tc, c+1) },\n",
    "          { case ((accF1, accE1, accTC1, c1), (accF2, accE2, accTC2, c2)) => (accF1+accF2, accE1+accE2, accTC1+accTC2, c1+c2) }\n",
    "        ).\n",
    "        mapValues { case (accF, accE, accTC,c) => (accF/c, accE/c, accTC/c, c) }. // ((k, class), (avgF, avgE, avgTC, c))\n",
    "        collect.foreach(println)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
